Source URL: https://cloud.google.com/bigquery/docs/use-spark

이 페이지는 Cloud Translation API [https://cloud.google.com/translate/?hl=ko]를 통해 번역되었습니다.
Switch to English
BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#before_you_begin]
가격 책정 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#pricing]
BigQuery Studio Python 노트북 열기 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#open-a-bigquery-python-notebook]
BigQuery Studio 노트북에서 Spark 세션 만들기 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#create-a-single-spark-session-in-a-bigquery-studio-notebook]
BigQuery Studio 노트북에서 PySpark 코드 작성 및 실행 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#write-and-run-pyspark-code-in-your-bigquery-studio-notebook]
BigQuery Studio 노트북에서 PySpark 코드 실행
bookmark_border
참고: 이제 Dataproc Serverless가 Google Cloud Apache Spark용 서버리스로 변경되었습니다. 업데이트될 때까지 일부 문서에서는 이전 이름을 사용합니다.
이 문서에서는 BigQuery Python 노트북에서 PySpark 코드를 실행하는 방법을 보여줍니다.
시작하기 전에
아직 만들지 않았다면 Google Cloud 프로젝트와 Cloud Storage 버킷 [https://cloud.google.com/storage/docs/xml-api/put-bucket-create?hl=ko]을 만듭니다.
프로젝트 설정
Sign in to your Google Cloud account. If you're new to Google Cloud, create an account [https://console.cloud.google.com/freetrial?hl=ko] to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.
Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector [https://console.cloud.google.com/projectselector2/home/dashboard?hl=ko]
Enable the Dataproc, BigQuery, and Cloud Storage APIs.
Enable the APIs [https://console.cloud.google.com/flows/enableapi?apiid=dataproc.googleapis.com%2Cbigquery.googleapis.com%2Cstorage-component.googleapis.com&hl=ko]
사용할 수 있는 버킷이 없으면 프로젝트에 Cloud Storage 버킷을 만듭니다 [https://cloud.google.com/storage/docs/creating-buckets?hl=ko].
노트북 설정하기
노트북 사용자 인증 정보: 기본적으로 노트북 세션은 사용자 인증 정보 [https://cloud.google.com/docs/authentication?hl=ko#user-accounts]를 사용합니다. 또는 세션 서비스 계정 [https://cloud.google.com/docs/authentication?hl=ko#service-accounts] 사용자 인증 정보를 사용할 수 있습니다.
사용자 사용자 인증 정보: 사용자 계정에는 다음 Identity and Access Management 역할이 있어야 합니다.
Dataproc 편집자 (roles/dataproc.editor 역할) [https://cloud.google.com/iam/docs/understanding-roles?hl=ko#dataproc.editor]
BigQuery Studio 사용자 (roles/bigquery.studioUser 역할) [https://cloud.google.com/iam/docs/understanding-roles?hl=ko#bigquery.studioUser]
세션 서비스 계정 [https://cloud.google.com/dataproc-serverless/docs/concepts/service-account?hl=ko]에 대한 서비스 계정 사용자 (roles/iam.serviceAccountUser) 역할 [https://cloud.google.com/iam/docs/service-account-permissions?hl=ko#user-role] 이 역할에는 서비스 계정을 가장하는 데 필요한 iam.serviceAccounts.actAs 권한이 포함되어 있습니다.
서비스 계정 사용자 인증 정보: 노트북 세션에 사용자 사용자 인증 정보 대신 서비스 계정 사용자 인증 정보를 지정하려면 세션 서비스 계정 [https://cloud.google.com/dataproc-serverless/docs/concepts/service-account?hl=ko]에 다음 역할이 있어야 합니다.
Dataproc 작업자 (roles/dataproc.worker 역할) [https://cloud.google.com/iam/docs/understanding-roles?hl=ko#dataproc.worker]
노트북 런타임: 다른 런타임을 선택하지 않는 한 노트북에서 기본 Vertex AI 런타임을 사용합니다. 자체 런타임을 정의하려면 Google Cloud 콘솔의 런타임 페이지 [https://console.cloud.google.com/vertex-ai/colab/runtimes?hl=ko]에서 런타임을 만드세요.
가격 책정
가격 정보는 BigQuery Notebook 런타임 가격 책정 [https://cloud.google.com/bigquery/pricing?hl=ko#external_service]을 참고하세요.
BigQuery Studio Python 노트북 열기
Google Cloud 콘솔에서 BigQuery 페이지로 이동합니다.
BigQuery로 이동 [https://console.cloud.google.com/bigquery?hl=ko]
세부정보 창의 탭 표시줄에서 + 기호 옆에 있는 arrow_drop_down 화살표를 클릭한 다음 노트북을 클릭합니다.
BigQuery Studio 노트북에서 Spark 세션 만들기
BigQuery Studio Python 노트북을 사용하여 Spark Connect [https://spark.apache.org/docs/latest/spark-connect-overview.html] 대화형 세션을 만들 수 있습니다. 각 BigQuery Studio 노트북에는 연결된 활성 Dataproc Serverless 세션이 하나만 있을 수 있습니다.
다음과 같은 방법으로 BigQuery Studio Python 노트북에서 Spark 세션을 만들 수 있습니다.
노트북에서 단일 세션을 구성하고 만듭니다.
Spark용 Dataproc Serverless Interactive 세션 템플릿 [https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates?hl=ko#create-dataproc-serverless-session-template]에서 Spark 세션을 구성한 다음 템플릿을 사용하여 노트북에서 세션을 구성하고 만듭니다. BigQuery는 템플릿화된 Spark 세션 탭에 설명된 대로 템플릿화된 세션 코딩을 시작하는 데 도움이 되는 Query using Spark 기능을 제공합니다.
--- 탭: 단일 세션 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#%EB%8B%A8%EC%9D%BC-%EC%84%B8%EC%85%98] ---
새 노트북에서 Spark 세션을 만들려면 다음 단계를 따르세요.


편집기 창의 탭 표시줄에서 + 기호 옆에 있는 arrow_drop_down 화살표 드롭다운을 클릭한 다음 노트북을 클릭합니다.




노트북 셀에서 다음 코드를 복사하고 실행하여 기본 Spark 세션을 구성하고 만듭니다.

from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session [https://cloud.google.com/python/docs/reference/dataproc/latest/google.cloud.dataproc_v1.types.Session.html?hl=ko]

import pyspark.sql.connect.functions as f

session = Session()

# Create the Spark session.
spark = (
   DataprocSparkSession.builder
     .appName("APP_NAME")
     .dataprocSessionConfig(session)
     .getOrCreate()
)

다음을 바꿉니다.


APP_NAME: 세션의 이름입니다(선택사항).
선택적 세션 설정: Dataproc API Session [https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.sessions?hl=ko#Session] 설정을 추가하여 세션을 맞춤설정할 수 있습니다. 다음은 몇 가지 예입니다.

RuntimeConfig [https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/RuntimeConfig?hl=ko]:




session.runtime_config.properties={spark.property.key1:VALUE_1,...,spark.property.keyN:VALUE_N}
session.runtime_config.container_image = path/to/container/image 

EnvironmentConfig [https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/EnvironmentConfig?hl=ko#ExecutionConfig]:




session.environment_config.execution_config.subnetwork_uri = "SUBNET_NAME"
session.environment_config.execution_config.ttl = {"seconds": VALUE}
session.environment_config.execution_config.service_account = SERVICE_ACCOUNT

--- 탭: 템플릿 기반 Spark 세션 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#%ED%85%9C%ED%94%8C%EB%A6%BF-%EA%B8%B0%EB%B0%98-spark-%EC%84%B8%EC%85%98] ---
노트북 셀에 코드를 입력하고 실행하여 기존 Dataproc Serverless 세션 템플릿 [https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates?hl=ko#create-dataproc-serverless-session-template]을 기반으로 Spark 세션을 만들 수 있습니다.
노트북 코드에서 제공하는 session 구성 설정은 세션 템플릿에 설정된 동일한 설정을 재정의합니다.

빠르게 시작하려면 Query using Spark 템플릿을 사용하여 Spark 세션 템플릿 코드로 노트북을 미리 채우세요.


편집기 창의 탭 표시줄에서 + 기호 옆에 있는 arrow_drop_down 화살표 드롭다운을 클릭한 다음 노트북을 클릭합니다.

  

템플릿으로 시작에서 Spark를 사용하여 쿼리를 클릭한 다음 템플릿 사용을 클릭하여 노트북에 코드를 삽입합니다.

  

참고 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#notes]에 설명된 대로 변수를 지정합니다.
노트북에 삽입된 추가 샘플 코드 셀을 삭제할 수 있습니다.

from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session [https://cloud.google.com/python/docs/reference/dataproc/latest/google.cloud.dataproc_v1.types.Session.html?hl=ko]
import pyspark.sql.connect.functions as f
session = Session()
# Configure the session with an existing session template.
session_template = "SESSION_TEMPLATE"
session.session_template = f"projects/{project}/locations/{location}/sessionTemplates/{session_template}"
# Create the Spark session.
spark = (
   DataprocSparkSession.builder
     .appName("APP_NAME")
     .dataprocSessionConfig(session)
     .getOrCreate()
)


다음을 바꿉니다.


PROJECT: 프로젝트 ID입니다. Google Cloud 콘솔 대시보드 [https://console.cloud.google.com/home/dashboard?hl=ko]의 프로젝트 정보 섹션에 나열됩니다.
LOCATION: 노트북 세션이 실행되는 Compute Engine 리전 [https://cloud.google.com/compute/docs/regions-zones?hl=ko#available]입니다. 제공되지 않으면 기본 위치는 노트북을 만드는 VM의 리전입니다.
SESSION_TEMPLATE: 기존 Dataproc 서버리스 대화형 세션 템플릿 [https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates?hl=ko#create-dataproc-serverless-session-template]의 이름입니다.
세션 구성 설정은 템플릿에서 가져옵니다.
템플릿은 다음 설정도 지정해야 합니다.


런타임 버전 2.3+ [https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-2.3?hl=ko]
노트북 유형: Spark Connect

예:





APP_NAME: 세션의 이름입니다(선택사항).
BigQuery Studio 노트북에서 PySpark 코드 작성 및 실행
노트북에서 Spark 세션을 만든 후 세션을 사용하여 노트북에서 Spark 노트북 코드를 실행합니다.
Spark Connect PySpark API 지원: Spark Connect 노트북 세션은 DataFrame [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html], Functions [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html], Column [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html]을 비롯한 대부분의 PySpark API [https://spark.apache.org/docs/latest/api/python/reference/index.html]를 지원하지만 SparkContext [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html], RDD [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html] 및 기타 PySpark API는 지원하지 않습니다. 자세한 내용은 Spark 3.5에서 지원되는 항목 [https://spark.apache.org/docs/latest/spark-connect-overview.html#what-is-supported]을 참고하세요.
팁: Spark SQL API 참조 [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html]를 확인하여 Spark Connect에서 API를 지원하는지 확인할 수 있습니다. 지원되는 API의 문서에는 'Supports Spark Connect' 메시지가 포함되어 있습니다.
Dataproc 전용 API: Dataproc은 addArtifacts 메서드를 확장하여 PyPI 패키지를 Spark 세션에 동적으로 추가하는 작업을 간소화합니다. version-scheme [https://packaging.python.org/en/latest/specifications/version-specifiers/#examples-of-compliant-version-schemes] 형식(pip install와 유사)으로 목록을 지정할 수 있습니다. 이렇게 하면 Spark Connect 서버가 모든 클러스터 노드에 패키지와 종속 항목을 설치하여 UDF의 작업자에게 제공됩니다.
textdistance 및 random2를 사용하는 UDF가 작업자 노드에서 실행되도록 지정된 textdistance 버전과 호환되는 최신 random2 라이브러리를 클러스터에 설치하는 예
spark.addArtifacts("textdistance==4.6.1", "random2", pypi=True)
노트북 코드 도움말: BigQuery Studio 노트북은 클래스 또는 메서드 이름 위로 포인터를 가져가면 코드 도움말을 제공하고, 코드를 입력할 때 코드 완성 도움말을 제공합니다.
다음 예에서는 DataprocSparkSession을 입력합니다. 이 클래스 이름 위로 포인터를 가져가면 코드 완성 및 문서 도움말이 표시됩니다.
BigQuery Studio 노트북 PySpark 예시
이 섹션에서는 다음 작업을 실행하는 PySpark 코드가 포함된 BigQuery Studio Python 노트북 예를 제공합니다.
공개 셰익스피어 데이터 세트에 대해 wordcount를 실행합니다.
BigLake metastore에 저장된 메타데이터로 Iceberg 테이블을 만듭니다.
--- 탭: Wordcount [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#wordcount] ---
다음 Pyspark 예시에서는 Spark 세션을 만든 다음 공개 bigquery-public-data.samples.shakespeare 데이터 세트에서 단어 발생 횟수를 계산합니다.
# Basic wordcount example
from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session [https://cloud.google.com/python/docs/reference/dataproc/latest/google.cloud.dataproc_v1.types.Session.html?hl=ko]
import pyspark.sql.connect.functions as f
session = Session()

# Create the Spark session.
spark = (
   DataprocSparkSession.builder
     .appName("APP_NAME")
     .dataprocSessionConfig(session)
     .getOrCreate()
)
# Run a wordcount on the public Shakespeare dataset.
df = spark.read.format("bigquery").option("table", "bigquery-public-data.samples.shakespeare").load()
words_df = df.select(f.explode(f.split(f.col("word"), " ")).alias("word"))
word_counts_df = words_df.filter(f.col("word") != "").groupBy("word").agg(f.count("*").alias("count")).orderBy("word")
word_counts_df.show()

다음을 바꿉니다.


APP_NAME: 세션의 이름입니다(선택사항).


출력:

셀 출력에 wordcount 출력의 샘플이 나열됩니다. Google Cloud 콘솔에서 세션 세부정보를 보려면 대화형 세션 세부정보 보기 링크를 클릭합니다.
Spark 세션을 모니터링하려면 세션 세부정보 페이지에서 Spark UI 보기를 클릭합니다.


  


Interactive Session Detail View: LINK
+------------+-----+
|        word|count|
+------------+-----+
|           '|   42|
|       ''All|    1|
|     ''Among|    1|
|       ''And|    1|
|       ''But|    1|
|    ''Gamut'|    1|
|       ''How|    1|
|        ''Lo|    1|
|      ''Look|    1|
|        ''My|    1|
|       ''Now|    1|
|         ''O|    1|
|      ''Od's|    1|
|       ''The|    1|
|       ''Tis|    4|
|      ''When|    1|
|       ''tis|    1|
|      ''twas|    1|
|          'A|   10|
|'ARTEMIDORUS|    1|
+------------+-----+
only showing top 20 rows

--- 탭: Iceberg 테이블 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#iceberg-%ED%85%8C%EC%9D%B4%EB%B8%94] ---
PySpark 코드를 실행하여 BigLake metastore 메타데이터로 Iceberg 테이블 만들기

다음 예시 코드에서는 BigLake metastore에 저장된 테이블 메타데이터로 sample_iceberg_table을 만든 다음 테이블을 쿼리합니다.
from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session [https://cloud.google.com/python/docs/reference/dataproc/latest/google.cloud.dataproc_v1.types.Session.html?hl=ko]
import pyspark.sql.connect.functions as f
# Create the Dataproc Serverless session.
session = Session()
# Set the session configuration for BigLake Metastore with the Iceberg environment.
project_id = "PROJECT"
region = "REGION"
subnet_name = "SUBNET_NAME"
location = "LOCATION"
session.environment_config.execution_config.subnetwork_uri = f"{subnet_name}"
warehouse_dir = "gs://BUCKET/WAREHOUSE_DIRECTORY"
catalog = "CATALOG_NAME"
namespace = "NAMESPACE"
session.runtime_config.properties[f"spark.sql.catalog.{catalog}"] = "org.apache.iceberg.spark.SparkCatalog"
session.runtime_config.properties[f"spark.sql.catalog.{catalog}.catalog-impl"] = "org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog"
session.runtime_config.properties[f"spark.sql.catalog.{catalog}.gcp_project"] = f"{project_id}"
session.runtime_config.properties[f"spark.sql.catalog.{catalog}.gcp_location"] = f"{location}"
session.runtime_config.properties[f"spark.sql.catalog.{catalog}.warehouse"] = f"{warehouse_dir}"
# Create the Spark Connect session.
spark = (
   DataprocSparkSession.builder
     .appName("APP_NAME")
     .dataprocSessionConfig(session)
     .getOrCreate()
)
# Create the namespace in BigQuery.
spark.sql(f"USE `{catalog}`;")
spark.sql(f"CREATE NAMESPACE IF NOT EXISTS `{namespace}`;")
spark.sql(f"USE `{namespace}`;")
# Create the Iceberg table.
spark.sql("DROP TABLE IF EXISTS `sample_iceberg_table`");
spark.sql("CREATE TABLE sample_iceberg_table (id int, data string) USING ICEBERG;")
spark.sql("DESCRIBE sample_iceberg_table;")
# Insert table data and query the table.
spark.sql("INSERT INTO sample_iceberg_table VALUES (1, \"first row\");")
# Alter table, then query and display table data and schema.
spark.sql("ALTER TABLE sample_iceberg_table ADD COLUMNS (newDoubleCol double);")
spark.sql("DESCRIBE sample_iceberg_table;")
df = spark.sql("SELECT * FROM sample_iceberg_table")
df.show()
df.printSchema()

참고:


PROJECT: 프로젝트 ID입니다. Google Cloud 콘솔 대시보드 [https://console.cloud.google.com/home/dashboard?hl=ko]의 프로젝트 정보 섹션에 나열됩니다.
REGION 및 SUBNET_NAME: Compute Engine 리전 [https://cloud.google.com/compute/docs/regions-zones?hl=ko#available]과 세션 리전의 서브넷 이름을 지정합니다.
Dataproc Serverless는 지정된 서브넷에서 비공개 Google 액세스 (PGA) [https://cloud.google.com/vpc/docs/private-google-access?hl=ko]를 사용 설정합니다.
LOCATION: 기본 BigQuery_metastore_config.location 및 spark.sql.catalog.{catalog}.gcp_location는 US이지만 지원되는 BigQuery 위치 [https://cloud.google.com/bigquery/docs/locations?hl=ko#supported_locations]를 선택할 수 있습니다.
BUCKET 및 WAREHOUSE_DIRECTORY: Iceberg 웨어하우스 디렉터리에 사용되는 Cloud Storage 버킷 및 폴더입니다.
CATALOG_NAME 및 NAMESPACE: Iceberg 카탈로그 이름과 네임스페이스가 결합되어 Iceberg 테이블(catalog.namespace.table_name)을 식별합니다.
APP_NAME: 세션의 이름입니다(선택사항).


셀 출력에는 추가된 열이 있는 sample_iceberg_table이 나열되고 Google Cloud 콘솔의 대화형 세션 세부정보 페이지로 연결되는 링크가 표시됩니다.
세션 세부정보 페이지에서 Spark UI 보기를 클릭하여 Spark 세션을 모니터링할 수 있습니다.


  


Interactive Session Detail View: LINK
+---+---------+------------+
| id|     data|newDoubleCol|
+---+---------+------------+
|  1|first row|        NULL|
+---+---------+------------+

root
 |-- id: integer (nullable = true)
 |-- data: string (nullable = true)
 |-- newDoubleCol: double (nullable = true)


BigQuery에서 테이블 세부정보 보기

BigQuery에서 Iceberg 테이블 세부정보를 확인하려면 다음 단계를 따르세요.


 Google Cloud 콘솔에서 BigQuery 페이지로 이동합니다.

BigQuery로 이동 [https://console.cloud.google.com/bigquery?hl=ko] 
프로젝트 리소스 창에서 프로젝트를 클릭한 다음 네임스페이스를 클릭하여 sample_iceberg_table 테이블을 나열합니다. 세부정보 표를 클릭하여 카탈로그 테이블 구성 열기 정보를 확인합니다.

입력 및 출력 형식은 Iceberg에서 사용하는 표준 Hadoop InputFormat 및 OutputFormat 클래스 형식입니다.

--- 탭: 기타 예 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#%EA%B8%B0%ED%83%80-%EC%98%88] ---
Pandas DataFrame (df)에서 Spark DataFrame (sdf)을 만듭니다.
sdf = spark.createDataFrame(df)
sdf.show()

Spark DataFrames에서 집계를 실행합니다.
from pyspark.sql import functions as F

sdf.groupby("segment").agg(
   F.mean("total_spend_per_user").alias("avg_order_value"),
   F.approx_count_distinct("user_id").alias("unique_customers")
).show()

Spark-BigQuery [https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=ko] 커넥터를 사용하여 BigQuery에서 읽습니다.
spark.conf.set("viewsEnabled","true")
spark.conf.set("materializationDataset","my-bigquery-dataset")

sdf = spark.read.format('bigquery') \
 .load(query)
Gemini Code Assist로 Spark 코드 작성
프리뷰
이 제품 또는 기능에는 서비스별 약관 [https://cloud.google.com/terms/service-terms?hl=ko#1]의 일반 서비스 약관 섹션에 있는 'GA 이전 제공 서비스 약관'이 적용됩니다. GA 이전 제품 및 기능은 '있는 그대로' 제공되며 지원이 제한될 수 있습니다. 자세한 내용은 출시 단계 설명 [https://cloud.google.com/products?hl=ko#product-launch-stages]을 참조하세요.
Gemini Code Assist에 노트북에서 PySpark 코드를 생성해 달라고 요청할 수 있습니다. Gemini Code Assist는 관련 BigQuery 및 Dataproc Metastore 테이블과 스키마를 가져와 사용하여 코드 응답을 생성합니다.
노트북에서 Gemini Code Assist 코드를 생성하려면 다음 단계를 따르세요.
툴바에서 + 코드를 클릭하여 새 코드 셀을 삽입합니다. 새 코드 셀에 Start coding or generate with AI이 표시됩니다. 생성을 클릭합니다.
생성 편집기에서 자연어 프롬프트를 입력한 다음 enter 아이콘을 클릭합니다. 프롬프트에 키워드 spark 또는 pyspark를 포함해야 합니다.
샘플 프롬프트:
create a spark dataframe from order_items and filter to orders created in 2024
샘플 출력:
spark.read.format("bigquery").option("table", "sqlgen-testing.pysparkeval_ecommerce.order_items").load().filter("year(created_at) = 2024").createOrReplaceTempView("order_items")
df = spark.sql("SELECT * FROM order_items")
Gemini Code Assist 코드 생성 도움말
Gemini Code Assist가 관련 테이블과 스키마를 가져오도록 하려면 Dataproc Metastore 인스턴스에 Data Catalog 동기화 [https://cloud.google.com/dataproc-metastore/docs/data-catalog-sync?hl=ko]를 사용 설정하세요.
사용자 계정에 Data Catalog 쿼리 테이블에 대한 액세스 권한이 있는지 확인합니다. 이를 위해 DataCatalog.Viewer 역할 [https://cloud.google.com/iam/docs/understanding-roles?hl=ko#datacatalog.viewer]을 할당합니다.
Spark 세션 종료
BigQuery Studio 노트북에서 Spark Connect 세션을 중지하려면 다음 작업 중 하나를 수행하면 됩니다.
노트북 셀에서 spark.stop()를 실행합니다.
노트북에서 런타임을 종료합니다.
런타임 선택기를 클릭한 다음 세션 관리를 클릭합니다.
활성 세션 대화상자에서 종료 아이콘을 클릭한 다음 종료를 클릭합니다.
BigQuery Studio 노트북 코드 오케스트레이션
다음과 같은 방법으로 BigQuery Studio 노트북 코드를 오케스트레이션할 수 있습니다.
Google Cloud 콘솔에서 노트북 코드를 예약합니다(노트북 가격 책정 [https://cloud.google.com/bigquery/pricing?hl=ko#external_service] 적용).
노트북 코드를 Dataproc 서버리스 배치 워크로드로 실행합니다(Dataproc 서버리스 가격 책정 [https://cloud.google.com/dataproc-serverless/pricing?hl=ko] 적용).
Google Cloud 콘솔에서 노트북 코드 예약
다음과 같은 방법으로 노트북 코드를 예약할 수 있습니다.
노트북 예약하기 [https://cloud.google.com/bigquery/docs/orchestrate-notebooks?hl=ko]
노트북 코드 실행이 워크플로의 일부인 경우 노트북을 파이프라인 [https://cloud.google.com/bigquery/docs/orchestrate-workflows?hl=ko]의 일부로 예약합니다.
노트북 코드를 Dataproc Serverless 일괄 워크로드로 실행
다음 단계를 완료하여 BigQuery Studio 노트북 코드를 Dataproc 서버리스 일괄 워크로드로 실행하세요.
로컬 터미널 또는 Cloud Shell [https://console.cloud.google.com/?cloudshell=true&hl=ko]의 파일에 노트북 코드를 다운로드합니다.
Cloud Shell에는 텍스트 편집기 및 기타 도구 [https://cloud.google.com/shell/docs/how-cloud-shell-works?hl=ko#tools]가 사전 설치되어 있고 Python 지원 [https://cloud.google.com/shell/docs/how-cloud-shell-works?hl=ko#language_support]이 내장되어 있으므로 Cloud Shell에 다운로드하여 작업하는 것이 좋습니다.
Google Cloud 콘솔의 BigQuery Studio 페이지 [https://console.cloud.google.com/bigquery?hl=ko]에 있는 탐색기 패널에서 노트북을 엽니다.
파일 메뉴에서 다운로드를 선택하여 노트북 코드를 다운로드한 다음 Download .py를 선택합니다.
requirements.txt 생성
.py 파일을 저장한 디렉터리에 pipreqs를 설치합니다.
pip install pipreqs
pipreqs를 실행하여 requirements.txt를 생성합니다.
pipreqs filename.py
Google Cloud CLI [https://cloud.google.com/sdk/gcloud?hl=ko]를 사용하여 로컬 requirements.txt 파일을 Cloud Storage의 버킷에 복사합니다.
gcloud storage cp requirements.txt gs://
BUCKET/
다운로드한 .py 파일을 수정하여 Spark 세션 코드를 업데이트합니다.
셸 스크립트 명령어를 삭제하거나 주석 처리합니다.
Spark 세션을 구성하는 코드를 삭제한 다음 구성 매개변수를 배치 워크로드 제출 매개변수로 지정합니다. (Spark 배치 워크로드 제출 [https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch?hl=ko#submit_a_spark_batch_workload] 참고)
예:
코드에서 다음 세션 서브넷 구성 줄을 삭제합니다.
session.environment_config.execution_config.subnetwork_uri = "{subnet_name}"
배치 워크로드를 실행 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#run-the-batch-workload]할 때 --subnet 플래그를 사용하여 서브넷을 지정합니다.
gcloud dataproc batches submit pyspark \
--subnet=SUBNET_NAME
간단한 세션 생성 코드 스니펫을 사용합니다.
간소화 전 다운로드된 노트북 코드 샘플
from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session
session = Session()
spark = DataprocSparkSession \
    .builder \
    .appName("CustomSparkSession")
    .dataprocSessionConfig(session) \
    .getOrCreate()
간소화된 후의 배치 워크로드 코드
from pyspark.sql import SparkSession
spark = SparkSession \
.builder \
.getOrCreate()
일괄 워크로드를 실행합니다.
자세한 내용은 Spark 배치 워크로드 제출 [https://cloud.google.com/bigquery/docs/dataproc-serverless/docs/quickstarts/spark-batch?hl=ko#submit_a_spark_batch_workload]을 참고하세요.
requirements.txt 파일이 포함된 Cloud Storage 버킷을 가리키는 --deps-bucket 플래그를 포함해야 합니다.
예:
gcloud dataproc batches submit pyspark 
FILENAME.py \
    --region=
REGION \
    --deps-bucket=
BUCKET \
    --version=2.3 
참고:
FILENAME: 다운로드하여 수정한 노트북 코드 파일의 이름입니다.
REGION: 클러스터가 있는 Compute Engine 리전 [https://cloud.google.com/compute/docs/regions-zones?hl=ko#available]
BUCKET requirements.txt 파일이 포함된 Cloud Storage 버킷의 이름입니다.
--version: Spark 런타임 버전 2.3 [https://cloud.google.com/bigquery/docs/dataproc-serverless/docs/concepts/versions/spark-runtime-2.3?hl=ko]이 선택되어 배치 워크로드를 실행합니다.
코드를 커밋합니다.
일괄 워크로드 코드를 테스트한 후 CI/CD 파이프라인의 일부로 GitHub, GitLab, Bitbucket과 같은 git 클라이언트를 사용하여 .ipynb 또는 .py 파일을 저장소에 커밋할 수 있습니다.
Cloud Composer로 일괄 워크로드를 예약합니다.
자세한 내용은 Cloud Composer로 Dataproc Serverless 워크로드 실행 [https://cloud.google.com/composer/docs/composer-2/run-dataproc-workloads?hl=ko]을 참고하세요.
노트북 오류 문제 해결
Spark 코드가 포함된 셀에서 오류가 발생하면 셀 출력에서 대화형 세션 세부정보 보기 링크를 클릭하여 오류를 해결할 수 있습니다 (단어 수 및 Iceberg 표 예 [https://cloud.google.com/bigquery/docs/use-spark?hl=ko#dataproc_serverless_bq_notebook-Wordcount] 참고).
노트북 코드 오류가 발생하면 Spark UI의 마지막 Spark 작업으로 이동하면 실패한 작업을 디버깅하는 데 도움이 되는 추가 정보가 제공되는 경우가 많습니다.
알려진 문제 및 해결 방법
오류: Python 버전 3.10로 생성된 노트북 런타임 [https://console.cloud.google.com/vertex-ai/colab/runtimes?hl=ko]이 Spark 세션에 연결하려고 하면 PYTHON_VERSION_MISMATCH 오류가 발생할 수 있습니다.
해결 방법: Python 버전 3.11로 런타임을 다시 만듭니다.
다음 단계
YouTube 동영상 데모: BigQuery와 통합된 Apache Spark의 강력한 기능 활용 [https://www.youtube.com/watch?v=DIZn6Nuur7k&hl=ko]
Dataproc에서 BigLake metastore 사용 [https://cloud.google.com/bigquery/docs/bqms-use-dataproc?hl=ko]
Dataproc Serverless에서 BigLake metastore 사용 [https://cloud.google.com/bigquery/docs/bqms-use-dataproc-serverless?hl=ko]
도움이 되었나요?
의견 보내기