Source URL: https://cloud.google.com/bigquery/docs/migration/snowflake-transfer

BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
개요 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#overview]
제한사항 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#limitations]
시작하기 전에 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#before_you_begin]
Google Cloud 프로젝트 준비 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#preparing-gcp-project]
필요한 BigQuery 역할 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#required-roles]
Amazon S3 버킷 준비 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#preparing-s3-bucket]
필요한 권한이 있는 Snowflake 사용자 만들기 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]
네트워크 정책 추가 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#add_network_policies]
Snowflake 전송 예약
bookmark_border
프리뷰
이 기능에는 서비스별 약관 [https://cloud.google.com/terms/service-terms?hl=ko#1]의 일반 서비스 약관 섹션에 있는 'GA 이전 제공 서비스 약관'이 적용됩니다. GA 이전 기능은 '있는 그대로' 제공되며 지원이 제한될 수 있습니다. 자세한 내용은 출시 단계 설명 [https://cloud.google.com/products?hl=ko#product-launch-stages]을 참조하세요.
참고: 이 기능에 대한 지원을 받거나 의견을 제공하거나 제한사항을 문의하려면 dts-migration-preview-support@google.com [mailto:dts-migration-preview-support@google.com]으로 문의하세요.
BigQuery Data Transfer Service에서 제공하는 Snowflake 커넥터를 사용하면 공개 IP 허용 목록을 사용하여 Snowflake에서 BigQuery로 데이터를 마이그레이션하는 자동 전송 작업을 예약하고 관리할 수 있습니다.
개요
Snowflake 커넥터는 Google Kubernetes Engine의 마이그레이션 에이전트와 연결되어 Snowflake에서 Snowflake가 호스팅되는 동일한 클라우드 제공업체 내 스테이징 영역으로의 로드 작업을 트리거합니다. AWS 호스팅 Snowflake 계정의 경우 데이터가 먼저 Amazon S3 버킷에 스테이징된 후 BigQuery Data Transfer Service를 통해 BigQuery로 전송됩니다.
다음 다이어그램에서는 데이터가 AWS 호스팅 Snowflake 데이터 웨어하우스에서 BigQuery로 전송되는 방식을 보여줍니다.
제한사항
Snowflake 커넥터를 사용하여 데이터를 전송할 때는 다음과 같은 제한사항이 적용됩니다.
데이터 전송은 AWS에서 호스팅되는 Snowflake 계정에서만 지원됩니다. Google Cloud 또는 Microsoft Azure에서 호스팅되는 Snowflake 계정의 데이터 전송은 지원되지 않습니다.
Snowflake 커넥터는 증분 데이터 전송을 지원하지 않습니다.
Snowflake 커넥터는 단일 Snowflake 데이터베이스 및 스키마 내 테이블에서만 전송을 지원합니다. Snowflake 데이터베이스 또는 스키마가 여러 개 있는 테이블에서 전송하려면 각 전송 작업을 별도로 설정하면 됩니다.
Snowflake에서 Amazon S3 버킷으로 데이터를 로드하는 속도는 이 전송에 선택한 Snowflake 웨어하우스에 따라 제한됩니다.
데이터는 BigQuery에 로드되기 전에 Parquet 데이터 형식으로 Snowflake에서 추출됩니다.
다음 Parquet 데이터 유형은 지원되지 않습니다.
TIMESTAMP_TZ, TIMESTAMP_LTZ
OBJECT, VARIANT, ARRAY
자세한 내용은 Snowflake 데이터 평가 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#assess-snowflake-data]를 참조하세요.
다음 Parquet 데이터 유형은 지원되지 않지만 변환될 수는 있습니다.
TIMESTAMP_NTZ
메타데이터를 생성하고 변환 엔진을 실행 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_metadata_and_run_translation_engine]할 때 DATETIME 변환 기본 동작을 TIMESTAMP로 재정의하려면 전역 유형 변환 구성 YAML [https://cloud.google.com/bigquery/docs/config-yaml-translation?hl=ko#global_type_conversion]을 사용합니다.
구성 YAML은 다음 예시와 비슷할 수 있습니다.
type: experimental_object_rewriter
global:
  typeConvert:
    datetime: TIMESTAMP
시작하기 전에
Snowflake 전송을 설정하기 전에 이 섹션에 나와 있는 모든 단계를 수행해야 합니다. 다음은 필요한 모든 단계의 목록입니다.
Google Cloud 프로젝트 준비 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#preparing-gcp-project]
필요한 BigQuery 역할 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#required-roles]
Amazon S3 버킷 준비 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#preparing-s3-bucket]
필요한 권한이 있는 Snowflake 사용자 만들기 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]
네트워크 정책 추가 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#add_network_policies]
메타데이터 생성 및 변환 엔진 실행 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_metadata_and_run_translation_engine]
Snowflake에서 지원되지 않는 데이터 유형 평가 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#assess-snowflake-data]
전송 정보 수집 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#gather_transfer_information]
Google Cloud 프로젝트 준비
다음 단계를 수행하여 Snowflake 전송에 사용할 Google Cloud 프로젝트를 만들고 구성합니다.
Google Cloud 프로젝트를 만들거나 [https://cloud.google.com/resource-manager/docs/creating-managing-projects?hl=ko] 기존 프로젝트를 선택합니다.
참고: 이 Snowflake 전송 중에 생성된 리소스를 유지하지 않으려면 기존 프로젝트를 선택하는 대신 새 Google Cloud 프로젝트를 만드세요. Snowflake 전송이 완료되면 프로젝트를 삭제할 수 있습니다.
BigQuery Data Transfer Service 사용 설정 [https://cloud.google.com/bigquery/docs/enable-transfer-service?hl=ko]에 필요한 모든 작업을 완료했는지 확인합니다.
데이터를 저장할 BigQuery 데이터 세트를 만듭니다 [https://cloud.google.com/bigquery/docs/datasets?hl=ko]. 테이블을 만들지 않아도 됩니다.
필요한 BigQuery 역할
전송을 만드는 데 필요한 권한을 얻으려면 관리자에게 BigQuery 관리자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.admin](roles/bigquery.admin) IAM 역할을 부여해 달라고 요청하세요. 역할 부여에 대한 자세한 내용은 프로젝트, 폴더, 조직에 대한 액세스 관리 [https://cloud.google.com/iam/docs/granting-changing-revoking-access?hl=ko]를 참조하세요.
이 사전 정의된 역할에는 전송을 만드는 데 필요한 권한이 포함되어 있습니다. 필요한 정확한 권한을 보려면 필수 권한 섹션을 펼치세요.
필수 권한
커스텀 역할 [https://cloud.google.com/iam/docs/creating-custom-roles?hl=ko]이나 다른 사전 정의된 역할 [https://cloud.google.com/iam/docs/roles-overview?hl=ko#predefined]을 사용하여 이 권한을 부여받을 수도 있습니다.
Amazon S3 버킷 준비
Snowflake 데이터 전송을 완료하려면 Amazon S3 버킷을 만든 후 Snowflake에서 쓰기 액세스를 허용하도록 구성해야 합니다.
Amazon S3 버킷을 만듭니다 [https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html]. Amazon S3 버킷은 데이터를 BigQuery에 로드하기 전에 스테이징하는 데 사용됩니다.
Snowflake에서 Amazon S3 버킷에 데이터를 외부 단계로 쓸 수 있도록 Snowflake 스토리지 통합 객체를 만들고 구성 [https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration]합니다.
Amazon S3 버킷에 대한 읽기 액세스를 허용하려면 다음도 수행해야 합니다.
전용 Amazon IAM 사용자 [https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html]를 만들고 AmazonS3ReadOnlyAccess [https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonS3ReadOnlyAccess.html] 정책을 부여합니다.
IAM 사용자의 Amazon 액세스 키 쌍을 만듭니다 [https://docs.aws.amazon.com/keyspaces/latest/devguide/create.keypair.html].
필요한 권한이 있는 Snowflake 사용자 만들기
Snowflake 전송 중에 Snowflake 커넥터는 JDBC 연결을 통해 Snowflake 계정에 연결됩니다. 데이터 전송을 수행하는 데 필요한 권한만 있는 커스텀 역할로 새 Snowflake 사용자를 만들어야 합니다.
  // Create and configure new role, 
MIGRATION_ROLE
  GRANT USAGE
    ON WAREHOUSE 
WAREHOUSE_NAME
    TO ROLE 
MIGRATION_ROLE;

  GRANT USAGE
    ON DATABASE 
DATABASE_NAME
    TO ROLE 
MIGRATION_ROLE;

  GRANT USAGE
    ON SCHEMA 
DATABASE_NAME.
SCHEMA_NAME
    TO ROLE 
MIGRATION_ROLE;

  // You can modify this to give select permissions for all tables in a schema
  GRANT SELECT
    ON TABLE 
DATABASE_NAME.
SCHEMA_NAME.
TABLE_NAME
    TO ROLE 
MIGRATION_ROLE;

  GRANT USAGE
    ON 
STORAGE_INTEGRATION_OBJECT_NAME
    TO ROLE 
MIGRATION_ROLE;
다음을 바꿉니다.
MIGRATION_ROLE: 만들려는 커스텀 역할의 이름
WAREHOUSE_NAME: 데이터 웨어하우스 이름
DATABASE_NAME: Snowflake 데이터베이스 이름
SCHEMA_NAME: Snowflake 스키마 이름
TABLE_NAME: 이 데이터 전송에 포함된 Snowflake의 이름
STORAGE_INTEGRATION_OBJECT_NAME: Snowflake 스토리지 통합 객체 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#storage-integration-bucket] 이름
인증에 사용할 키 쌍 생성
Snowflake에서 단일 요소 비밀번호 로그인 지원을 중단 [https://docs.snowflake.com/en/user-guide/security-mfa-rollout]했으므로 인증에 키 쌍을 사용하는 것이 좋습니다.
암호화되거나 암호화되지 않은 RSA 키 쌍을 생성한 후 공개 키를 Snowflake 사용자에게 할당하여 키 쌍을 구성할 수 있습니다. 자세한 내용은 키 쌍 인증 구성 [https://docs.snowflake.com/en/user-guide/key-pair-auth#configuring-key-pair-authentication]을 참조하세요.
네트워크 정책 추가
공개 연결의 경우 Snowflake 계정은 기본적으로 데이터베이스 사용자 인증 정보로 공개 연결을 허용합니다. 하지만 Snowflake 커넥터가 계정에 연결되지 않도록 하는 네트워크 규칙이나 정책을 구성했을 수 있습니다. 이 경우 필요한 IP 주소를 허용 목록에 추가해야 합니다.
다음 표는 공개 전송에 사용되는 리전 및 멀티 리전 위치의 IP 주소 목록입니다. 데이터 세트 위치에 해당하는 IP 주소만 추가하거나 표에 나와 있는 모든 IP 주소를 추가할 수 있습니다. BigQuery Data Transfer Service 데이터 전송을 위해 Google에서 예약한 IP 주소입니다.
IP 주소를 허용 목록에 추가하려면 다음을 수행합니다.
type = IPV4인 네트워크 규칙을 만듭니다 [https://docs.snowflake.com/en/sql-reference/sql/create-network-rule]. BigQuery Data Transfer Service는 JDBC 연결을 사용하여 Snowflake 계정에 연결합니다.
이전에 만든 네트워크 규칙과 다음 표의 IP 주소로 네트워크 정책을 만듭니다 [https://docs.snowflake.com/en/sql-reference/sql/create-network-policy].
주의: BigQuery와 Snowflake 간의 통신은 다음과 같은 Google 소유 IP 주소를 통해 이루어집니다. 그러나 Amazon S3에서 BigQuery로의 데이터 이동은 공용 인터넷을 통해 이루어집니다.
리전 위치
리전 설명 리전 이름 IP 주소
미주
오하이오 주 콜럼부스 us-east5 34.162.72.184
34.162.173.185
34.162.205.205
34.162.81.45
34.162.182.149
34.162.59.92
34.162.157.190
34.162.191.145
Dallas us-south1 34.174.172.89
34.174.40.67
34.174.5.11
34.174.96.109
34.174.148.99
34.174.176.19
34.174.253.135
34.174.129.163
아이오와 us-central1 34.121.70.114
34.71.81.17
34.122.223.84
34.121.145.212
35.232.1.105
35.202.145.227
35.226.82.216
35.225.241.102
라스베이거스 us-west4 34.125.53.201
34.125.69.174
34.125.159.85
34.125.152.1
34.125.195.166
34.125.50.249
34.125.68.55
34.125.91.116
로스앤젤레스 us-west2 35.236.59.167
34.94.132.139
34.94.207.21
34.94.81.187
34.94.88.122
35.235.101.187
34.94.238.66
34.94.195.77
멕시코 northamerica-south1 34.51.6.35
34.51.7.113
34.51.12.83
34.51.10.94
34.51.11.219
34.51.11.52
34.51.2.114
34.51.15.251
몬트리올 northamerica-northeast1 34.95.20.253
35.203.31.219
34.95.22.233
34.95.27.99
35.203.12.23
35.203.39.46
35.203.116.49
35.203.104.223
북 버지니아 us-east4 35.245.95.250
35.245.126.228
35.236.225.172
35.245.86.140
35.199.31.35
35.199.19.115
35.230.167.48
35.245.128.132
35.245.111.126
35.236.209.21
오리건 us-west1 35.197.117.207
35.199.178.12
35.197.86.233
34.82.155.140
35.247.28.48
35.247.31.246
35.247.106.13
34.105.85.54
솔트레이크시티 us-west3 34.106.37.58
34.106.85.113
34.106.28.153
34.106.64.121
34.106.246.131
34.106.56.150
34.106.41.31
34.106.182.92
상파울루 southamerica-east1 35.199.88.228
34.95.169.140
35.198.53.30
34.95.144.215
35.247.250.120
35.247.255.158
34.95.231.121
35.198.8.157
산티아고 southamerica-west1 34.176.188.48
34.176.38.192
34.176.205.134
34.176.102.161
34.176.197.198
34.176.223.236
34.176.47.188
34.176.14.80
사우스캐롤라이나 us-east1 35.196.207.183
35.237.231.98
104.196.102.222
35.231.13.201
34.75.129.215
34.75.127.9
35.229.36.137
35.237.91.139
토론토 northamerica-northeast2 34.124.116.108
34.124.116.107
34.124.116.102
34.124.116.80
34.124.116.72
34.124.116.85
34.124.116.20
34.124.116.68
유럽
벨기에 europe-west1 35.240.36.149
35.205.171.56
34.76.234.4
35.205.38.234
34.77.237.73
35.195.107.238
35.195.52.87
34.76.102.189
베를린 europe-west10 34.32.28.80
34.32.31.206
34.32.19.49
34.32.33.71
34.32.15.174
34.32.23.7
34.32.1.208
34.32.8.3
핀란드 europe-north1 35.228.35.94
35.228.183.156
35.228.211.18
35.228.146.84
35.228.103.114
35.228.53.184
35.228.203.85
35.228.183.138
프랑크푸르트 europe-west3 35.246.153.144
35.198.80.78
35.246.181.106
35.246.211.135
34.89.165.108
35.198.68.187
35.242.223.6
34.89.137.180
런던 europe-west2 35.189.119.113
35.189.101.107
35.189.69.131
35.197.205.93
35.189.121.178
35.189.121.41
35.189.85.30
35.197.195.192
마드리드 europe-southwest1 34.175.99.115
34.175.186.237
34.175.39.130
34.175.135.49
34.175.1.49
34.175.95.94
34.175.102.118
34.175.166.114
밀라노 europe-west8 34.154.183.149
34.154.40.104
34.154.59.51
34.154.86.2
34.154.182.20
34.154.127.144
34.154.201.251
34.154.0.104
네덜란드 europe-west4 35.204.237.173
35.204.18.163
34.91.86.224
34.90.184.136
34.91.115.67
34.90.218.6
34.91.147.143
34.91.253.1
파리 europe-west9 34.163.76.229
34.163.153.68
34.155.181.30
34.155.85.234
34.155.230.192
34.155.175.220
34.163.68.177
34.163.157.151
스톡홀름 europe-north2 34.51.133.48
34.51.136.177
34.51.128.140
34.51.141.252
34.51.139.127
34.51.142.55
34.51.134.218
34.51.138.9
토리노 europe-west12 34.17.15.186
34.17.44.123
34.17.41.160
34.17.47.82
34.17.43.109
34.17.38.236
34.17.34.223
34.17.16.47
바르샤바 europe-central2 34.118.72.8
34.118.45.245
34.118.69.169
34.116.244.189
34.116.170.150
34.118.97.148
34.116.148.164
34.116.168.127
취리히 europe-west6 34.65.205.160
34.65.121.140
34.65.196.143
34.65.9.133
34.65.156.193
34.65.216.124
34.65.233.83
34.65.168.250
아시아 태평양
델리 asia-south2 34.126.212.96
34.126.212.85
34.126.208.224
34.126.212.94
34.126.208.226
34.126.212.232
34.126.212.93
34.126.212.206
홍콩 asia-east2 34.92.245.180
35.241.116.105
35.220.240.216
35.220.188.244
34.92.196.78
34.92.165.209
35.220.193.228
34.96.153.178
자카르타 asia-southeast2 34.101.79.105
34.101.129.32
34.101.244.197
34.101.100.180
34.101.109.205
34.101.185.189
34.101.179.27
34.101.197.251
멜버른 australia-southeast2 34.126.196.95
34.126.196.106
34.126.196.126
34.126.196.96
34.126.196.112
34.126.196.99
34.126.196.76
34.126.196.68
뭄바이 asia-south1 34.93.67.112
35.244.0.1
35.200.245.13
35.200.203.161
34.93.209.130
34.93.120.224
35.244.10.12
35.200.186.100
오사카 asia-northeast2 34.97.94.51
34.97.118.176
34.97.63.76
34.97.159.156
34.97.113.218
34.97.4.108
34.97.119.140
34.97.30.191
서울 asia-northeast3 34.64.152.215
34.64.140.241
34.64.133.199
34.64.174.192
34.64.145.219
34.64.136.56
34.64.247.158
34.64.135.220
싱가포르 asia-southeast1 34.87.12.235
34.87.63.5
34.87.91.51
35.198.197.191
35.240.253.175
35.247.165.193
35.247.181.82
35.247.189.103
시드니 australia-southeast1 35.189.33.150
35.189.38.5
35.189.29.88
35.189.22.179
35.189.20.163
35.189.29.83
35.189.31.141
35.189.14.219
타이완 asia-east1 35.221.201.20
35.194.177.253
34.80.17.79
34.80.178.20
34.80.174.198
35.201.132.11
35.201.223.177
35.229.251.28
35.185.155.147
35.194.232.172
도쿄 asia-northeast1 34.85.11.246
34.85.30.58
34.85.8.125
34.85.38.59
34.85.31.67
34.85.36.143
34.85.32.222
34.85.18.128
34.85.23.202
34.85.35.192
중동
Dammam me-central2 34.166.20.177
34.166.10.104
34.166.21.128
34.166.19.184
34.166.20.83
34.166.18.138
34.166.18.48
34.166.23.171
도하 me-central1 34.18.48.121
34.18.25.208
34.18.38.183
34.18.33.25
34.18.21.203
34.18.21.80
34.18.36.126
34.18.23.252
텔아비브 me-west1 34.165.184.115
34.165.110.74
34.165.174.16
34.165.28.235
34.165.170.172
34.165.187.98
34.165.85.64
34.165.245.97
아프리카
요하네스버그 africa-south1 34.35.11.24
34.35.10.66
34.35.8.32
34.35.3.248
34.35.2.113
34.35.5.61
34.35.7.53
34.35.3.17
다중 리전 위치
멀티 리전 설명 멀티 리전 이름 IP 주소
유럽 연합 회원국 [https://europa.eu/european-union/about-eu/countries_en]의 데이터 센터1 EU 34.76.156.158
34.76.156.172
34.76.136.146
34.76.1.29
34.76.156.232
34.76.156.81
34.76.156.246
34.76.102.206
34.76.129.246
34.76.121.168
미국의 데이터 센터 US 35.185.196.212
35.197.102.120
35.185.224.10
35.185.228.170
35.197.5.235
35.185.206.139
35.197.67.234
35.197.38.65
35.185.202.229
35.185.200.120
1 EU 멀티 리전에 있는 데이터는 europe-west2(런던) 또는 europe-west6(취리히) 데이터 센터에 저장되지 않습니다.
메타데이터 생성 및 변환 엔진 실행
Snowflake용 BigQuery Data Transfer Service 커넥터는 Snowflake 테이블을 BigQuery로 마이그레이션할 때 스키마 매핑에 BigQuery 마이그레이션 서비스 변환 엔진을 사용합니다. Snowflake 데이터 전송을 완료하려면 먼저 변환에 사용할 메타데이터를 생성한 후 변환 엔진을 실행해야 합니다.
Snowflake용 dwh-migration-tool을 실행합니다. 자세한 내용은 변환 및 평가를 위한 메타데이터 생성 [https://cloud.google.com/bigquery/docs/generate-metadata?hl=ko#snowflake]을 참조하세요.
생성된 metadata.zip 파일을 Cloud Storage 버킷에 업로드합니다. metadata.zip 파일은 변환 엔진의 입력으로 사용됩니다.
target_types 필드를 metadata로 지정하여 일괄 변환 서비스를 실행합니다. 자세한 내용은 변환 API를 사용한 SQL 쿼리 변환 [https://cloud.google.com/bigquery/docs/api-sql-translator?hl=ko]을 참조하세요.
다음은 Snowflake의 일괄 변환을 실행하는 명령어의 예시입니다.
  curl -d "{
  \"name\": \"sf_2_bq_translation\",
  \"displayName\": \"Snowflake to BigQuery Translation\",
  \"tasks\": {
      string: {
        \"type\": \"Snowflake2BigQuery_Translation\",
        \"translation_details\": {
            \"target_base_uri\": \"gs://sf_test_translation/output\",
            \"source_target_mapping\": {
              \"source_spec\": {
                  \"base_uri\": \"gs://sf_test_translation/input\"
              }
            },
            \"target_types\": \"metadata\",
        }
      }
  },
  }" \
  -H "Content-Type:application/json" \
  -H "Authorization: Bearer TOKEN" -X POST https://bigquerymigration.googleapis.com/v2alpha/projects/project_id/locations/location/workflows
BigQuery의 SQL 변환 페이지 [https://console.cloud.google.com/bigquery/migrations/batch-translation?hl=ko]에서 이 명령어 상태를 확인할 수 있습니다. 일괄 변환 작업 출력은 gs://translation_target_base_uri/metadata/config/에 저장됩니다.
필수 서비스 계정 권한
Snowflake 전송에서는 지정된 Cloud Storage 경로의 변환 엔진 출력에서 데이터를 읽는 데 서비스 계정이 사용됩니다. 서비스 계정에 storage.objects.get 및 storage.objects.list 권한을 부여해야 합니다.
서비스 계정이 BigQuery 데이터 전송을 만든 프로젝트와 다른 Google Cloud 프로젝트에 있으면 교차 프로젝트 서비스 계정 승인도 사용 설정 [https://cloud.google.com/bigquery/docs/enable-transfer-service?hl=ko#cross-project_service_account_authorization]해야 합니다.
자세한 내용은 BigQuery IAM 역할 및 권한 [https://cloud.google.com/bigquery/docs/access-control?hl=ko]을 참조하세요.
Snowflake 데이터 평가
BigQuery는 Snowflake의 데이터를 Cloud Storage에 Parquet 파일로 씁니다. Parquet 파일은 TIMESTAMP_TZ 및 TIMESTAMP_LTZ [https://community.snowflake.com/s/article/How-To-Unload-Timestamp-data-in-a-Parquet-file] 데이터 유형을 지원하지 않습니다. 데이터에 이러한 유형이 포함되어 있으면 CSV 파일로 Amazon S3에 내보낸 후 CSV 파일을 BigQuery로 가져올 수 있습니다. 자세한 내용은 Amazon S3 전송 개요 [https://cloud.google.com/bigquery/docs/s3-transfer-intro?hl=ko]를 참조하세요.
전송 정보 수집
BigQuery Data Transfer Service로 마이그레이션을 설정하는 데 필요한 정보를 수집합니다.
Snowflake 계정 식별자(Snowflake 계정 URL의 프리픽스). 예를 들면 ACCOUNT_IDENTIFIER.snowflakecomputing.com입니다.
Snowflake 데이터베이스에 대한 적절한 권한이 있는 사용자 이름과 연결된 비공개 키. 데이터 전송을 실행하는 데 필요한 권한 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]만 있으면 됩니다.
전송에 사용할 Amazon S3 버킷의 URI 및 AWS 사용자 액세스 키 쌍 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_key_pair]. 불필요한 비용이 청구되지 않도록 이 버킷에 수명 주기 정책 [https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html]을 설정하는 것이 좋습니다.
변환 엔진에서 가져온 스키마 매핑 파일 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_metadata_and_run_translation_engine]을 저장한 Cloud Storage 버킷의 URI.
Snowflake 전송 설정
다음 옵션 중 하나를 선택합니다.
--- 탭: 콘솔 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#%EC%BD%98%EC%86%94] ---
Google Cloud 콘솔에서 데이터 전송 페이지로 이동합니다.

데이터 전송으로 이동 [https://console.cloud.google.com/bigquery/transfers?hl=ko] 
add 전송 만들기를 클릭합니다.
소스 유형 섹션의 소스 목록에서 Snowflake 마이그레이션을 선택합니다.
전송 구성 이름 섹션에서 전송 이름(예: My migration)을 표시 이름 필드에 입력합니다. 표시 이름은 나중에 수정해야 할 경우를 대비해 전송을 식별할 수 있는 값이면 됩니다.
대상 설정 섹션의 데이터 세트 목록에서 만든 데이터 세트 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#preparing-gcp-project]를 선택합니다.
데이터 소스 세부정보 섹션에서 다음을 수행합니다.


계정 식별자에 Snowflake 계정의 고유 식별자를 입력합니다. 이 고유 식별자는 조직 이름과 계정 이름의 조합입니다. 식별자는 Snowflake 계정 URL 프리픽스이며 전체 URL이 아닙니다. 예를 들면 ACCOUNT_IDENTIFIER.snowflakecomputing.com입니다.
사용자 이름에 Snowflake 테이블을 전송하기 위해 데이터베이스에 액세스하는 데 사용되는 사용자 인증 정보와 승인이 있는 Snowflake 사용자의 사용자 이름을 입력합니다. 이 전송을 위해 만든 사용자 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]를 사용하는 것이 좋습니다.
인증 메커니즘에서 Snowflake 사용자 인증 방법을 선택합니다. 자세한 내용은 인증을 위한 키 쌍 생성 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_key_pair_for_authentication]을 참조하세요.
비밀번호에 Snowflake 사용자 비밀번호를 입력합니다. 인증 메커니즘 필드에서 PASSWORD를 선택한 경우 이 필드는 필수입니다.
비공개 키에 Snowflake 사용자와 연결된 공개 키 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]와 연결된 비공개 키를 입력합니다.
인증 메커니즘 필드에서 KEY_PAIR를 선택한 경우 이 필드는 필수입니다.
비공개 키가 암호로 암호화된 경우 비공개 키가 암호화됨 필드를 선택합니다.
비공개 키 암호에 암호화된 비공개 키의 암호를 입력합니다. 인증 메커니즘 및 비공개 키가 암호화됨 필드에서 KEY_PAIR를 선택한 경우 이 필드는 필수입니다.
웨어하우스에 이 데이터 전송 실행에 사용되는 웨어하우스 [https://docs.snowflake.com/en/user-guide/warehouses-tasks]를 입력합니다.
서비스 계정에 이 데이터 전송에 사용할 서비스 계정을 입력합니다. 서비스 계정은 전송 구성과 대상 데이터 세트가 생성된 동일한Google Cloud 프로젝트에 속해야 합니다. 서비스 계정에는 storage.objects.list 및 storage.objects.get 필수 권한 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#required_service_account_permissions]이 있어야 합니다.
데이터베이스에 이 데이터 전송에 포함된 테이블이 있는 Snowflake 데이터베이스의 이름을 입력합니다.
스키마에 이 데이터 전송에 포함된 테이블이 포함된 Snowflake 스키마의 이름을 입력합니다.
테이블 이름 패턴에 스키마의 테이블 이름과 일치하는 이름이나 패턴을 입력하여 전송할 테이블을 지정합니다. 정규 표현식을 사용하여 패턴을 지정할 수 있습니다(예: table1_regex;table2_regex). 이 패턴은 Java 정규 표현식 문법을 따라야 합니다. 예를 들면 다음과 같습니다.


lineitem;ordertb는 lineitem 및 ordertb 테이블과 일치합니다.
.*은 모든 테이블을 찾습니다.

변환 출력 GCS 경로에 변환 엔진의 스키마 매핑 파일 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_metadata_and_run_translation_engine]이 포함된 Cloud Storage 폴더의 경로를 지정합니다.


경로는 translation_target_base_uri/metadata/config/db/schema/ 형식을 따라야 하며 /로 끝나야 합니다.

스토리지 통합 객체 이름에 Snowflake 스토리지 통합 객체 이름을 입력합니다. 이 경우는 S3입니다.
클라우드 제공업체에서 AWS를 선택합니다.
GCS URI, Azure 스토리지 계정 이름, Azure 컨테이너 이름, Azure SAS의 경우 이 필드를 비워 둡니다.
Amazon S3 URI에 스테이징 영역으로 사용할 S3 버킷의 URI [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_uri]를 입력합니다.
액세스 키 ID 및 보안 비밀 액세스 키에 액세스 키 쌍 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_key_pair]을 입력합니다.

선택사항: 알림 옵션 섹션에서 다음을 수행합니다.


전환을 클릭하여 이메일 알림을 사용 설정합니다. 이 옵션을 사용 설정하면 전송 실행이 실패할 때 전송 관리자에게 이메일 알림이 발송됩니다.
Pub/Sub 주제 선택에서 주제 [https://cloud.google.com/pubsub/docs/overview?hl=ko#types] 이름을 선택하거나 주제 만들기를 클릭합니다. 이 옵션은 전송에 대한 Pub/Sub 실행 알림 [https://cloud.google.com/bigquery/docs/transfer-run-notifications?hl=ko]을 구성합니다.

저장을 클릭합니다.
 Google Cloud 콘솔에 이 전송의 리소스 이름을 포함하여 모든 전송 설정 세부사항이 표시됩니다.

--- 탭: bq [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#bq] ---
bq mk 명령어를 입력하고 전송 생성 플래그 --transfer_config를 지정합니다. 다음 플래그도 필요합니다.


--project_id
--data_source
--target_dataset
--display_name
--params


bq mk \
    --transfer_config \
    --project_id=project_id \
    --data_source=data_source \
    --target_dataset=dataset \
    --display_name=name \
    --service_account_name=service_account \
    --params='parameters'

다음을 바꿉니다.


project_id: Google Cloud 프로젝트 ID입니다. --project_id를 지정하지 않으면 기본 프로젝트가 사용됩니다.
data_source: 데이터 소스(snowflake_migration)
dataset: 전송 구성에 사용할 BigQuery 대상 데이터 세트
name: 전송 구성의 표시 이름입니다. 전송 이름은 나중에 수정해야 할 경우를 대비해 전송을 식별할 수 있는 값이면 됩니다.
service_account: (선택사항) 전송을 인증하는 데 사용되는 서비스 계정 이름. 전송을 만드는 데 사용한 것과 동일한 project_id에서 서비스 계정을 소유해야 하며 이 계정에 모든 필요한 역할 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#required-roles]이 있어야 합니다.
parameters: JSON 형식으로 생성된 전송 구성의 매개변수. 예를 들면 --params='{"param":"param_value"}'입니다.


Snowflake 전송 구성에 필요한 파라미터는 다음과 같습니다.


account_identifier: Snowflake 계정의 고유 식별자를 지정합니다. 이 고유 식별자는 조직 이름과 계정 이름의 조합입니다. 식별자는 Snowflake 계정 URL 프리픽스이며 전체 URL이 아닙니다. 예를 들면 account_identifier.snowflakecomputing.com입니다.
username: Snowflake 테이블을 전송하기 위해 데이터베이스에 액세스하는 데 사용되는 사용자 인증 정보와 승인이 있는 Snowflake 사용자의 사용자 이름을 지정합니다.
auth_mechanism: Snowflake 사용자 인증 방법을 지정합니다.
  지원되는 값은 PASSWORD 및 KEY_PAIR입니다. 자세한 내용은 인증을 위한 키 쌍 생성 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_key_pair_for_authentication]을 참조하세요.
password: Snowflake 사용자 비밀번호를 지정합니다. auth_mechanism 필드에 PASSWORD를 지정한 경우 이 필드는 필수입니다.
private_key: Snowflake 사용자와 연결된 공개 키 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#create-snowflake-user]와 연결된 비공개 키를 지정합니다.
  auth_mechanism 필드에 KEY_PAIR를 지정한 경우 이 필드는 필수입니다.
is_private_key_encrypted: 비공개 키가 암호로 암호화된 경우 true를 지정합니다.
private_key_passphrase: 암호화된 비공개 키의 암호를 지정합니다. auth_mechanism 필드에 KEY_PAIR를 지정하고 is_private_key_encrypted 필드에 true를 지정한 경우 이 필드는 필수입니다.
warehouse: 이 데이터 전송 실행에 사용되는 웨어하우스 [https://docs.snowflake.com/en/user-guide/warehouses-tasks]를 지정합니다.
service_account: 이 데이터 전송에서 사용할 서비스 계정을 지정합니다. 서비스 계정은 전송 구성과 대상 데이터 세트가 생성된 동일한 Google Cloud 프로젝트에 속해야 합니다. 서비스 계정에는 storage.objects.list 및 storage.objects.get 필수 권한 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#required_service_account_permissions]이 있어야 합니다.
database: 이 데이터 전송에 포함된 테이블이 있는 Snowflake 데이터베이스의 이름을 지정합니다.
schema: 이 데이터 전송에 포함된 테이블이 있는 Snowflake 스키마의 이름을 지정합니다.
table_name_patterns: 스키마에서 테이블 이름과 일치하는 이름이나 패턴을 입력하여 전송할 테이블을 지정합니다. 정규 표현식을 사용하여 패턴을 지정할 수 있습니다(예: table1_regex;table2_regex). 이 패턴은 Java 정규 표현식 문법을 따라야 합니다. 예를 들면 다음과 같습니다.


lineitem;ordertb는 lineitem 및 ordertb 테이블과 일치합니다.
.*은 모든 테이블을 찾습니다.

지정된 스키마에서 모든 테이블을 마이그레이션하려면 이 필드도 비워 두면 됩니다.

translation_output_gcs_path: 변환 엔진의 스키마 매핑 파일 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#generate_metadata_and_run_translation_engine]이 포함된 Cloud Storage 폴더의 경로를 지정합니다.


경로는 gs://translation_target_base_uri/metadata/config/db/schema/ 형식을 따라야 하며 /로 끝나야 합니다.

storage_integration_object_name: Snowflake 스토리지 통합 객체 이름을 지정합니다. 이 경우는 S3입니다.
cloud_provider: AWS를 지정합니다.
amazon_s3_uri: 스테이징 영역으로 사용할 S3 버킷 URI [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_uri]를 지정합니다.
aws_access_key_id: 액세스 키 쌍 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_key_pair]을 지정합니다.
aws_secret_access_key: 액세스 키 쌍 [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#snowflake_key_pair]을 지정합니다.


예를 들어 다음 명령어는 your_bq_dataset 대상 데이터 세트와 ID가 your_project_id인 프로젝트를 사용하여 snowflake transfer config Snowflake 전송을 만듭니다.

  PARAMS='{
  "account_identifier": "your_account_identifier",
  "auth_mechanism": "KEY_PAIR",
  "aws_access_key_id": "your_access_key_id",
  "aws_secret_access_key": "your_aws_secret_access_key",
  "cloud_provider": "AWS",
  "database": "your_sf_database",
  "private_key": "-----BEGIN PRIVATE KEY----- privatekey\nseparatedwith\nnewlinecharacters=-----END PRIVATE KEY-----",
  "schema": "your_snowflake_schema",
  "service_account": "your_service_account",
  "storage_integration_object_name": "your_storage_integration_object",
  "staging_s3_uri": "s3://your/s3/bucket/uri",
  "table_name_patterns": ".*",
  "translation_output_gcs_path": "gs://sf_test_translation/output/metadata/config/database_name/schema_name/",
  "username": "your_sf_username",
  "warehouse": "your_warehouse"
}'

bq mk --transfer_config \
    --project_id=your_project_id \
    --target_dataset=your_bq_dataset \
    --display_name='snowflake transfer config' \
    --params="$PARAMS" \
    --data_source=snowflake_migration
참고: 명령줄 도구를 사용하여 알림을 구성할 수 없습니다.

--- 탭: API [https://cloud.google.com/bigquery/docs/migration/snowflake-transfer?hl=ko#api] ---
projects.locations.transferConfigs.create [https://cloud.google.com/bigquery-transfer/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/create?hl=ko] 메서드를 사용하고 TransferConfig [https://cloud.google.com/bigquery-transfer/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs?hl=ko#TransferConfig] 리소스의 인스턴스를 지정합니다.
같은 Snowflake 테이블에 전송을 여러 개 만들거나 같은 전송 구성을 여러 번 실행하면 기존 BigQuery 대상 테이블의 데이터를 덮어씁니다.
할당량 및 한도
BigQuery에는 테이블별 로드 작업당 15TB의 로드 할당량이 있습니다. 내부적으로 Snowflake는 테이블 데이터를 압축하므로 내보낸 테이블 크기는 Snowflake에서 보고한 테이블 크기보다 큽니다. 15TB보다 큰 테이블을 마이그레이션하려면 dts-migration-preview-support@google.com [mailto:dts-migration-preview-support@google.com]에 문의하세요.
Amazon S3 일관성 모델 [https://cloud.google.com/bigquery/docs/s3-transfer-intro?hl=ko#consistency_considerations]로 인해 일부 파일이 BigQuery로 전송되지 않을 수 있습니다.
가격 책정
BigQuery Data Transfer Service 가격에 대한 자세한 내용은 가격 책정 [https://cloud.google.com/bigquery-transfer/pricing?hl=ko] 페이지를 참조하세요.
Snowflake 웨어하우스와 Amazon S3 버킷이 서로 다른 리전에 있는 경우 Snowflake 데이터 전송을 실행하면 Snowflake에서 이그레스 요금을 적용합니다. Snowflake 웨어하우스와 Amazon S3 버킷 모두 같은 리전에 있으면 Snowflake 데이터 전송에 이그레스 요금이 청구되지 않습니다.
데이터가 AWS에서 Google Cloud로 전송되면 클라우드 간 이그레스 요금이 적용됩니다.
다음 단계
BigQuery Data Transfer Service [https://cloud.google.com/bigquery-transfer/docs/transfer-service-overview?hl=ko] 자세히 알아보기
SQL 일괄 변환 [https://cloud.google.com/bigquery/docs/batch-sql-translator?hl=ko]으로 SQL 코드 마이그레이션
도움이 되었나요?
의견 보내기