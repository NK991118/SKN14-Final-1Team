Source URL: https://cloud.google.com/bigquery/docs/connect-to-spark

BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#before_you_begin]
위치 고려사항 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#location-considerations]
연결 만들기 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#create-spark-connection]
서비스 계정에 액세스 권한 부여 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#grant-access]
사용자와 연결 공유 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#share_connections]
다음 단계 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#whats_next]
Apache Spark에 연결
bookmark_border
BigQuery 관리자는 데이터 분석가가 Apache Spark의 저장 프로시저를 실행 [https://cloud.google.com/bigquery/docs/spark-procedures?hl=ko]하도록 연결 [https://cloud.google.com/bigquery/docs/connections-api-intro?hl=ko]을 만들 수 있습니다.
시작하기 전에
BigQuery Connection API 사용 설정하기
API 사용 설정하기 [https://console.cloud.google.com/apis/library/bigqueryconnection.googleapis.com?hl=ko]
Spark 연결을 만드는 데 필요한 권한을 얻으려면 관리자에게 프로젝트에 대한 BigQuery 연결 관리자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.connectionAdmin](roles/bigquery.connectionAdmin) IAM 역할을 부여해 달라고 요청하세요. 역할 부여에 대한 자세한 내용은 프로젝트, 폴더, 조직에 대한 액세스 관리 [https://cloud.google.com/iam/docs/granting-changing-revoking-access?hl=ko]를 참조하세요.
커스텀 역할 [https://cloud.google.com/iam/docs/creating-custom-roles?hl=ko]이나 다른 사전 정의된 역할 [https://cloud.google.com/iam/docs/roles-overview?hl=ko#predefined]을 통해 필요한 권한을 얻을 수도 있습니다.
(선택사항): Dataproc Metastore [https://cloud.google.com/dataproc-metastore/docs/overview?hl=ko]를 사용하여 메타데이터를 관리하려면 Dataproc Metastore 서비스를 만들 [https://cloud.google.com/dataproc-metastore/docs/create-service?hl=ko]었는지 확인하세요.
(선택사항): Spark 기록 서버 웹 인터페이스를 사용하여 작업 기록을 보려면 [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko#spark_history_server_web_interface] Dataproc 영구 기록 서버(PHS) [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko#create_a_phs_cluster]를 만들어야 합니다.
위치 고려사항
데이터 위치를 선택할 때는 다음 사항을 고려해야 합니다.
멀티 리전
동일한 넓은 지리적 지역에 있는 Google Cloud 리소스를 지정해야 합니다.
BigQuery US 멀티 리전의 연결은 us-central1, us-east4, us-west2 등 미국 내 어느 단일 리전의 Spark 기록 서버 [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko] 또는 Dataproc Metastore [https://cloud.google.com/dataproc-metastore/docs/overview?hl=ko]를 참조할 수 있습니다.
BigQuery EU 멀티 리전의 연결은 europe-north1 또는 europe-west3 등 유럽 연합 회원국 [https://europa.eu/european-union/about-eu/countries_en]의 Spark 기록 서버 또는 Dataproc Metastore를 참조할 수 있습니다.
단일 리전
단일 리전의 연결은 같은 리전의 Google Cloud리소스만 참조할 수 있습니다. 예를 들어 단일 리전 us-east4의 연결은 us-east4의 Spark 기록 서버 또는 Dataproc Metastore만 참조할 수 있습니다.
연결 만들기
다음 옵션 중 하나를 선택합니다.
--- 탭: 콘솔 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#%EC%BD%98%EC%86%94] ---
BigQuery 페이지로 이동합니다.

BigQuery로 이동 [https://console.cloud.google.com/bigquery?hl=ko] 
탐색기 창에서 add 데이터 추가를 클릭합니다.

데이터 추가 대화상자가 열립니다.
필터링 기준 창의 데이터 소스 유형 섹션에서 데이터베이스를 선택합니다.

또는 데이터 소스 검색 필드에 Spark를 입력할 수 있습니다.
추천 데이터 소스 섹션에서 Apache Spark를 클릭합니다.
Apache Spark: BigQuery 제휴 솔루션 카드를 클릭합니다.
외부 데이터 소스 창에서 다음 정보를 입력합니다.


연결 유형 목록에서 Apache Spark를 선택합니다.
연결 ID 필드에 연결 이름을 입력합니다(예: spark_connection).
데이터 위치 목록에서 리전을 선택합니다.


BigQuery를 지원하는 리전 및 멀티 리전 [https://cloud.google.com/bigquery/docs/locations?hl=ko]에서 연결을 만들 수 있습니다.
자세한 내용은 위치 고려사항 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#location-considerations]을 참조하세요.


(선택사항): Metastore 서비스 목록에서 Dataproc Metastore [https://cloud.google.com/dataproc-metastore/docs/overview?hl=ko]를 선택합니다.
(선택사항): 기록 서버 클러스터 필드에 Dataproc 영구 기록 서버 [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko#create_a_phs_cluster]를 입력합니다.

연결 만들기를 클릭합니다.
연결로 이동을 클릭합니다.
연결 정보 창에서 다음 단계에 사용할 서비스 계정 ID를 복사합니다.

--- 탭: bq [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#bq] ---
명령줄 환경에서 bq mk 명령어 [https://cloud.google.com/bigquery/docs/reference/bq-cli-reference?hl=ko#bq_mk]를 사용하여 연결을 만듭니다.

bq mk --connection --connection_type='SPARK' \
 --properties=PROPERTIES \
 --project_id=PROJECT_ID \
 --location=LOCATION
 CONNECTION_ID


다음을 바꿉니다.


PROPERTIES: JSON 형식으로 연결별 매개변수를 제공하는 키-값 쌍입니다.

예를 들면 다음과 같습니다.

--properties='{
"metastoreServiceConfig": {"metastoreService": "METASTORE_SERVICE_NAME"},
"sparkHistoryServerConfig": {"dataprocCluster": "DATAPROC_CLUSTER_NAME"}
}'


다음을 바꿉니다.


METASTORE_SERVICE_NAME: gRPC 네트워크 구성을 사용하는 Dataproc Metastore [https://cloud.google.com/dataproc-metastore/docs/endpoint-protocol?hl=ko#grpc_network_configuration](예: projects/my-project-id/locations/us-central1/services/my-service)

자세한 내용은 엔드포인트 프로토콜을 사용하여 저장된 Hive 메타스토어 메타데이터 [https://cloud.google.com/dataproc-metastore/docs/endpoint-protocol?hl=ko]에 액세스하는 방법을 참조하세요.
DATAPROC_CLUSTER_NAME: Spark 기록 서버 구성(예: projects/my-project-id/regions/us-central1/clusters/my-cluster)

자세한 내용은 영구 기록 서버 클러스터 만들기 [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko#create_a_phs_cluster]를 참조하세요.

PROJECT_ID: Google Cloud 프로젝트 ID
LOCATION: 연결을 저장할 위치(예: US)
CONNECTION_ID: 연결 ID(예: myconnection).

 Google Cloud 콘솔에서 연결 세부정보를 볼 때 [https://cloud.google.com/bigquery/docs/working-with-connections?hl=ko#view-connections] 연결 ID는 연결 ID에 표시되는 정규화된 연결 ID의 마지막 섹션에 있는 값입니다(예: projects/.../locations/.../connections/myconnection).

다른 단계에 필요하므로 서비스 계정 ID를 검색하고 복사합니다.

bq show --location=LOCATION --connection PROJECT_ID.LOCATION.CONNECTION_ID


출력은 다음과 비슷합니다.

Connection myproject.us.myconnection

       name           type                    properties
---------------------- ------- ---------------------------------------------------
myproject.us.myconnection  SPARK   {"serviceAccountId": "bqserver@example.iam.gserviceaccount.com"}
연결 관리 방법에 대한 자세한 설명은 연결 관리 [https://cloud.google.com/bigquery/docs/working-with-connections?hl=ko]를 참조하세요.
서비스 계정에 액세스 권한 부여
Apache Spark의 저장 프러시저가 Google Cloud리소스에 액세스하도록 허용하려면 저장 프로시저의 연결과 관련된 서비스 계정에 필요한 IAM 권한을 부여해야 합니다. 또는 데이터 액세스를 위해 커스텀 서비스 계정 [https://cloud.google.com/bigquery/docs/spark-procedures?hl=ko#use_a_custom_service_account]을 사용할 수 있습니다.
BigQuery에서 데이터를 읽고 쓰려면 서비스 계정에 다음 IAM 권한을 부여해야 합니다.
BigQuery 테이블에 대한 bigquery.tables.* 권한
프로젝트에 대한 bigquery.readsessions.* 권한
roles/bigquery.admin IAM 역할에는 서비스 계정이 BigQuery에서 데이터를 읽고 쓰는 데 필요한 권한이 포함되어 있습니다.
참고: 저장 프로시저가 데이터를 임시 Cloud Storage 버킷에 작성한 다음 Cloud Storage 데이터를 BigQuery에 로드 [https://cloud.google.com/bigquery/docs/batch-loading-data?hl=ko]하면 서비스 계정에 프로젝트에 대한 bigquery.jobs.create 권한을 부여해야 합니다. BigQuery의 IAM 역할과 권한에 대한 상세 설명은 IAM으로 액세스 제어 [https://cloud.google.com/bigquery/access-control?hl=ko]를 참조하세요.
Cloud Storage에서 데이터를 읽고 쓰려면 서비스 계정에 Cloud Storage 객체에 대한 storage.objects.* 권한을 부여해야 합니다.
roles/storage.objectAdmin IAM 역할에는 서비스 계정이 Cloud Storage에서 데이터를 읽고 쓰는 데 필요한 권한이 포함되어 있습니다.
연결을 만들 때 Dataproc Metastore를 지정한 경우 BigQuery가 메타스토어 구성에 대한 세부정보를 검색하려면 서비스 계정에 Dataproc Metastore에 대한 metastore.services.get 권한을 부여해야 합니다.
사전 정의된 roles/metastore.metadataViewer 역할에는 서비스 계정이 메타스토어 구성에 대한 세부정보를 검색하는 데 필요한 권한이 포함되어 있습니다.
또한 저장 프로시져가 Dataproc Metastore(hive.metastore.warehouse.dir)의 Hive 웨어하우스 디렉터리에 액세스할 수 있도록 서비스 계정에 Cloud Storage 버킷에 대한 roles/storage.objectAdmin 역할을 부여해야 합니다. 저장 프로시져가 메타스토어에서 작업을 수행하는 경우 추가 권한을 부여해야 할 수 있습니다. Dataproc Metastore의 IAM 역할 및 권한에 대한 자세한 내용은 Dataproc Metastore 사전 정의된 역할 및 권한 [https://cloud.google.com/dataproc-metastore/docs/iam-roles?hl=ko]을 참조하세요.
연결을 만들 때 Dataproc 영구 기록 서버를 지정하는 경우 서비스 계정에 다음 역할을 부여해야 합니다.
dataproc.clusters.get 권한을 포함하는 Dataproc 영구 기록 서버에 대한 roles/dataproc.viewer 역할
Dataproc 영구 기록 서버를 만들 때 spark:spark.history.fs.logDirectory 속성에 지정하는 Cloud Storage 버킷에 대한 roles/storage.objectAdmin 역할
자세한 내용은 Dataproc 영구 기록 서버 [https://cloud.google.com/dataproc/docs/concepts/jobs/history-server?hl=ko#create_a_phs_cluster]와 Dataproc 역할 및 권한 [https://cloud.google.com/dataproc/docs/concepts/iam/iam?hl=ko]을 참조하세요.
사용자와 연결 공유
데이터를 쿼리하고 연결을 관리할 수 있도록 사용자에게 다음 역할을 부여할 수 있습니다.
roles/bigquery.connectionUser: 사용자가 연결을 사용하여 외부 데이터 소스에 연결하고 쿼리를 실행할 수 있습니다.
roles/bigquery.connectionAdmin: 사용자가 연결을 관리할 수 있습니다.
BigQuery의 IAM 역할과 권한에 대한 자세한 내용은 사전 정의된 역할 및 권한 [https://cloud.google.com/bigquery/access-control?hl=ko]을 참조하세요.
다음 옵션 중 하나를 선택합니다.
--- 탭: 콘솔 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#%EC%BD%98%EC%86%94] ---
BigQuery 페이지로 이동합니다.

BigQuery로 이동 [https://console.cloud.google.com/bigquery?hl=ko] 

이러한 연결은 프로젝트의 외부 연결 그룹에 나열됩니다.
탐색기 창에서 프로젝트 이름 > 외부 연결 > 연결을 클릭합니다.
세부정보 창에서 공유를 클릭하여 연결을 공유합니다.
다음 작업을 수행합니다.


연결 권한 대화상자에서 주 구성원을 추가하거나 수정하여 다른 주 구성원과 연결을 공유합니다.
저장을 클릭합니다.

--- 탭: bq [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#bq] ---
bq 명령줄 도구로 연결을 공유할 수 없습니다.
연결을 공유하려면 Google Cloud 콘솔이나 BigQuery Connections API 메서드를 사용하여 연결을 공유합니다.

--- 탭: API [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#api] ---
BigQuery 연결 REST API 참조 섹션의 projects.locations.connections.setIAM 메서드 [https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections?hl=ko#methods]를 사용하고 policy 리소스의 인스턴스를 지정합니다.

--- 탭: 자바 [https://cloud.google.com/bigquery/docs/connect-to-spark?hl=ko#%EC%9E%90%EB%B0%94] ---
이 샘플을 사용해 보기 전에 BigQuery 빠른 시작: 클라이언트 라이브러리 사용 [https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries?hl=ko]의 Java 설정 안내를 따르세요.
        
      
      
  자세한 내용은 BigQuery Java API 참고 문서 [https://cloud.google.com/java/docs/reference/google-cloud-bigquery/latest/overview?hl=ko]를 확인하세요.
  
    
    
      BigQuery에 인증하려면 애플리케이션 기본 사용자 인증 정보를 설정합니다.
      자세한 내용은 클라이언트 라이브러리의 인증 설정 [https://cloud.google.com/bigquery/docs/authentication?hl=ko#client-libs]을 참조하세요.
      
    
      






    
  
  
  
  




















  





  
    
  
  











  









  




  



  


  import com.google.api.resourcenames.ResourceName [https://cloud.google.com/java/docs/reference/api-common/latest/com.google.api.resourcenames.ResourceName.html?hl=ko];
import com.google.cloud.bigquery.connection.v1.ConnectionName [https://cloud.google.com/java/docs/reference/google-cloud-bigqueryconnection/latest/com.google.cloud.bigquery.connection.v1.ConnectionName.html?hl=ko];
import com.google.cloud.bigqueryconnection.v1.ConnectionServiceClient [https://cloud.google.com/java/docs/reference/google-cloud-bigqueryconnection/latest/com.google.cloud.bigqueryconnection.v1.ConnectionServiceClient.html?hl=ko];
import com.google.iam.v1.Binding [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Binding.html?hl=ko];
import com.google.iam.v1.Policy [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Policy.html?hl=ko];
import com.google.iam.v1.SetIamPolicyRequest [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.SetIamPolicyRequest.html?hl=ko];
import java.io.IOException;

// Sample to share connections
public class ShareConnection {

  public static void main(String[] args) throws IOException {
    // TODO(developer): Replace these variables before running the sample.
    String projectId = "MY_PROJECT_ID";
    String location = "MY_LOCATION";
    String connectionId = "MY_CONNECTION_ID";
    shareConnection(projectId, location, connectionId);
  }

  static void shareConnection(String projectId, String location, String connectionId)
      throws IOException {
    try (ConnectionServiceClient [https://cloud.google.com/java/docs/reference/google-cloud-bigqueryconnection/latest/com.google.cloud.bigqueryconnection.v1.ConnectionServiceClient.html?hl=ko] client = ConnectionServiceClient [https://cloud.google.com/java/docs/reference/google-cloud-bigqueryconnection/latest/com.google.cloud.bigqueryconnection.v1.ConnectionServiceClient.html?hl=ko].create()) {
      ResourceName [https://cloud.google.com/java/docs/reference/api-common/latest/com.google.api.resourcenames.ResourceName.html?hl=ko] resource = ConnectionName [https://cloud.google.com/java/docs/reference/google-cloud-bigqueryconnection/latest/com.google.cloud.bigquery.connection.v1.ConnectionName.html?hl=ko].of(projectId, location, connectionId);
      Binding [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Binding.html?hl=ko] binding =
          Binding [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Binding.html?hl=ko].newBuilder()
              .addMembers [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Binding.Builder.html?hl=ko#com_google_iam_v1_Binding_Builder_addMembers_java_lang_String_]("group:example-analyst-group@google.com")
              .setRole("roles/bigquery.connectionUser")
              .build();
      Policy [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Policy.html?hl=ko] policy = Policy [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Policy.html?hl=ko].newBuilder().addBindings [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.Policy.Builder.html?hl=ko#com_google_iam_v1_Policy_Builder_addBindings_com_google_iam_v1_Binding_](binding).build();
      SetIamPolicyRequest [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.SetIamPolicyRequest.html?hl=ko] request =
          SetIamPolicyRequest [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.SetIamPolicyRequest.html?hl=ko].newBuilder()
              .setResource(resource.toString())
              .setPolicy [https://cloud.google.com/java/docs/reference/proto-google-iam-v1/latest/com.google.iam.v1.SetIamPolicyRequest.Builder.html?hl=ko#com_google_iam_v1_SetIamPolicyRequest_Builder_setPolicy_com_google_iam_v1_Policy_](policy)
              .build();
      client.setIamPolicy(request);
      System.out.println("Connection shared successfully");
    }
  }
}
다음 단계
다양한 연결 유형 [https://cloud.google.com/bigquery/docs/connections-api-intro?hl=ko] 알아보기
연결 관리 [https://cloud.google.com/bigquery/docs/working-with-connections?hl=ko] 알아보기
Apache Spark의 저장 프로시저 생성 [https://cloud.google.com/bigquery/docs/spark-procedures?hl=ko] 방법 알아보기
저장 프로시저 관리 [https://cloud.google.com/bigquery/docs/routines?hl=ko] 방법 알아보기
도움이 되었나요?
의견 보내기