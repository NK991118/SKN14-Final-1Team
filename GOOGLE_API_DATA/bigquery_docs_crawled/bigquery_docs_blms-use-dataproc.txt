Source URL: https://cloud.google.com/bigquery/docs/blms-use-dataproc

이 페이지는 Cloud Translation API [https://cloud.google.com/translate/?hl=ko]를 통해 번역되었습니다.
Switch to English
BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#before-you-begin]
필요한 역할 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#required-roles]
일반 워크플로 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#workflow-to-dataproc]
BigLake Metastore를 Spark에 연결 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#connect-biglake]
Iceberg 카탈로그 플러그인 다운로드 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#download-iceberg-plugin]
Dataproc 클러스터 구성 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#create-dataproc-cluster]
Spark 작업 제출 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#submit-spark-commands]
BigLake Metastore를 Flink에 연결 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#connect-biglake-flink]
Dataproc에서 BigLake Metastore 사용
bookmark_border
이 문서에서는 Compute Engine의 Dataproc [https://cloud.google.com/dataproc/docs?hl=ko]에서 BigLake metastore를 사용하는 방법을 설명합니다. 이 연결은 Apache Spark 또는 Apache Flink와 같은 오픈소스 소프트웨어 엔진에서 작동하는 단일 공유 메타스토어를 제공합니다.
시작하기 전에
Google Cloud 프로젝트에 결제를 사용 설정합니다. 프로젝트에 결제가 사용 설정되어 있는지 확인 [https://cloud.google.com/billing/docs/how-to/verify-billing-enabled?hl=ko]하는 방법을 알아보세요.
BigQuery 및 Dataproc API를 사용 설정합니다.
API 사용 설정 [https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com%2Cdataproc.googleapis.com&hl=ko]
선택사항: BigLake Metastore의 작동 방식 [https://cloud.google.com/bigquery/docs/about-blms?hl=ko]과 이를 사용해야 하는 이유를 알아봅니다.
필요한 역할
BigLake metastore를 메타데이터 저장소로 사용하여 Spark 또는 Flink 및 Dataproc를 사용하는 데 필요한 권한을 얻으려면 관리자에게 다음 IAM 역할을 부여해 달라고 요청하세요.
Dataproc 클러스터를 만듭니다. 프로젝트의 Compute Engine 기본 서비스 계정에 대한 Dataproc 작업자 [https://cloud.google.com/iam/docs/roles-permissions/dataproc?hl=ko#dataproc.worker] (roles/dataproc.worker)
Spark 또는 Flink에서 BigLake metastore 테이블을 만듭니다.
프로젝트의 Dataproc VM 서비스 계정에 대한 Dataproc 작업자 [https://cloud.google.com/iam/docs/roles-permissions/dataproc?hl=ko#dataproc.worker] (roles/dataproc.worker)
프로젝트의 Dataproc VM 서비스 계정에 대한 BigQuery 데이터 편집자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.dataEditor] (roles/bigquery.dataEditor)
프로젝트의 Dataproc VM 서비스 계정에 대한 스토리지 객체 관리자 [https://cloud.google.com/iam/docs/roles-permissions/storage?hl=ko#storage.objectAdmin] (roles/storage.objectAdmin)
BigQuery에서 BigLake metastore 테이블을 쿼리합니다.
프로젝트에 대한 BigQuery 데이터 뷰어 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.dataViewer] (roles/bigquery.dataViewer)
프로젝트에 대한 BigQuery 사용자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.user] (roles/bigquery.user)
프로젝트의 스토리지 객체 뷰어 [https://cloud.google.com/iam/docs/roles-permissions/storage?hl=ko#storage.objectViewer] (roles/storage.objectViewer)
역할 부여에 대한 자세한 내용은 프로젝트, 폴더, 조직에 대한 액세스 관리 [https://cloud.google.com/iam/docs/granting-changing-revoking-access?hl=ko]를 참조하세요.
커스텀 역할 [https://cloud.google.com/iam/docs/creating-custom-roles?hl=ko]이나 다른 사전 정의된 역할 [https://cloud.google.com/iam/docs/roles-overview?hl=ko#predefined]을 통해 필요한 권한을 얻을 수도 있습니다.
일반 워크플로
BigLake 메타 스토어와 함께 Compute Engine에서 Dataproc을 사용하려면 다음 일반 단계를 따르세요.
Dataproc 클러스터를 만들거나 기존 클러스터를 구성합니다.
Spark 또는 Flink와 같은 선호하는 오픈소스 소프트웨어 엔진에 연결합니다.
JAR 파일을 사용하여 클러스터에 Apache Iceberg 카탈로그 플러그인을 설치합니다.
사용 중인 오픈소스 소프트웨어 엔진에 따라 필요에 따라 BigLake Metastore 리소스를 만들고 관리합니다.
BigQuery에서 BigLake Metastore 리소스에 액세스하고 사용합니다.
BigLake Metastore를 Spark에 연결
다음 안내에서는 대화형 Spark SQL을 사용하여 Dataproc을 BigLake metastore에 연결하는 방법을 보여줍니다.
Iceberg 카탈로그 플러그인 다운로드
BigLake Metastore를 Dataproc 및 Spark와 연결하려면 BigLake Metastore Iceberg 카탈로그 플러그인 jar 파일을 사용해야 합니다.
이 파일은 Dataproc 이미지 버전 2.2에 기본적으로 포함되어 있습니다. Dataproc 클러스터가 인터넷에 직접 액세스할 수 없는 경우 플러그인을 다운로드하여 Dataproc 클러스터가 액세스할 수 있는 Cloud Storage 버킷에 업로드해야 합니다.
BigLake Metastore Iceberg 카탈로그 플러그인 다운로드 [https://storage.googleapis.com/spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar]
Dataproc 클러스터 구성
BigLake 메타스토어에 연결하기 전에 Dataproc 클러스터를 설정해야 합니다.
이렇게 하려면 새 클러스터를 만들거나 기존 클러스터를 사용하면 됩니다. 그런 다음 이 클러스터를 사용하여 대화형 Spark SQL을 실행하고 BigLake metastore 리소스를 관리합니다.
클러스터가 생성된 리전의 서브넷에 비공개 Google 액세스(PGA) [https://cloud.google.com/vpc/docs/private-google-access?hl=ko]가 사용 설정되어 있어야 합니다. 기본적으로 2.2 (기본값) 이상 이미지 버전으로 생성된 Dataproc 클러스터 VM에는 내부 IP 주소만 [https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network?hl=ko#create-a-dataproc-cluster-with-internal-IP-addresses-only] 있습니다. 클러스터 VM이 Google API와 통신할 수 있도록 하려면 클러스터가 생성된 리전의 default (또는 해당하는 경우 사용자 지정 네트워크 이름) 네트워크 서브넷에서 비공개 Google 액세스를 사용 설정 [https://cloud.google.com/vpc/docs/configure-private-google-access?hl=ko#config-pga]합니다.
이 가이드의 Zeppelin 웹 인터페이스 예시를 실행하려면 Zeppelin 선택적 구성요소가 사용 설정된 Dataproc 클러스터를 사용하거나 만들어야 합니다.
--- 탭: 새 클러스터 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#%EC%83%88-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0] ---
새 Dataproc 클러스터를 만들려면 다음 gcloud
dataproc  clusters create [https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create?hl=ko] 명령어를 실행합니다. 이 구성에는 BigLake Metastore를 사용하는 데 필요한 설정이 포함되어 있습니다.

gcloud dataproc clusters create CLUSTER_NAME \
  --project=PROJECT_ID \
  --region=LOCATION \
  --optional-components=ZEPPELIN \
  --enable-component-gateway \
  --single-node

다음을 바꿉니다.


CLUSTER_NAME: Dataproc 클러스터의 이름입니다.
PROJECT_ID: 클러스터를 만드는 Google Cloud 프로젝트의 ID입니다.
LOCATION: 클러스터를 만드는 Google Cloud 리전입니다.

--- 탭: 기존 클러스터 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#%EA%B8%B0%EC%A1%B4-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0] ---
기존 클러스터를 구성하려면 클러스터에 다음 Iceberg Spark 런타임을 추가합니다.
org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1

다음 옵션 중 하나를 사용하여 런타임을 추가할 수 있습니다.


초기화 스크립트 생성될 때 실행되는 맞춤 초기화 스크립트에 런타임 종속 항목을 추가 [https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions?hl=ko]합니다.

스크립트에 런타임 종속 항목을 추가한 후 안내에 따라 클러스터를 다시 만들고 업데이트 [https://cloud.google.com/dataproc/docs/guides/recreate-cluster?hl=ko]합니다.
수동 설치. Iceberg 카탈로그 플러그인 JAR [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#download-iceberg-plugin] 파일을 수동으로 추가하고 클러스터에 런타임을 포함하도록 Spark 속성을 구성합니다.
Spark 작업 제출
Spark 작업을 제출하려면 다음 방법 중 하나를 사용하세요.
--- 탭: gcloud CLI [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#gcloud-cli] ---
gcloud dataproc jobs submit spark-sql \
--project=PROJECT_ID \
--cluster=CLUSTER_NAME \
--region==REGION \
--jars=https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar,gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar \
--properties=spark.sql.catalog.CATALOG_NAME=org.apache.iceberg.spark.SparkCatalog, \
spark.sql.catalog.CATALOG_NAME.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog, \
spark.sql.catalog.CATALOG_NAME.gcp_project=PROJECT_ID, \
spark.sql.catalog.CATALOG_NAME.gcp_location=LOCATION, \
spark.sql.catalog.CATALOG_NAME.warehouse=WAREHOUSE_DIRECTORY \
--execute="SPARK_SQL_COMMAND"


다음을 바꿉니다.


PROJECT_ID: Dataproc 클러스터가 포함된 Google Cloud 프로젝트의 ID입니다.
CLUSTER_NAME: Spark SQL 작업을 실행하는 데 사용하는 Dataproc 클러스터의 이름입니다.
REGION: 클러스터가 있는 Compute Engine 리전 [https://cloud.google.com/compute/docs/regions-zones?hl=ko#available]입니다.
LOCATION: BigQuery 리소스의 위치입니다.
CATALOG_NAME: SQL 작업과 함께 사용하는 Spark 카탈로그의 이름
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 포함된 Cloud Storage 폴더
이 값은 gs://로 시작합니다.
SPARK_SQL_COMMAND: 실행할 Spark SQL 쿼리입니다. 이 쿼리에는 리소스를 만드는 명령어가 포함되어 있습니다. 예를 들어 네임스페이스와 테이블을 만듭니다.

--- 탭: 대화형 Spark [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#%EB%8C%80%ED%99%94%ED%98%95-spark] ---
Spark에 연결하고 카탈로그 플러그인 설치

BigLake Metastore용 카탈로그 플러그인을 설치하려면 SSH를 사용하여 Dataproc 클러스터에 연결합니다.


 Google Cloud 콘솔에서 VM 인스턴스 [https://console.cloud.google.com/compute/instances?hl=ko] 페이지로 이동합니다.
Dataproc VM 인스턴스에 연결하려면 가상 머신 인스턴스 목록에서 SSH를 클릭합니다. 출력은 다음과 비슷합니다.
Connected, host fingerprint: ssh-rsa ...
Linux cluster-1-m 3.16.0-0.bpo.4-amd64 ...
...
example-cluster@cluster-1-m:~$

터미널에서 다음 BigLake 메타 스토어 초기화 명령어를 실행합니다.

spark-sql \
--jars https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar,gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar \
--conf spark.sql.catalog.CATALOG_NAME=org.apache.iceberg.spark.SparkCatalog \
--conf spark.sql.catalog.CATALOG_NAME.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog \
--conf spark.sql.catalog.CATALOG_NAME.gcp_project=PROJECT_ID \
--conf spark.sql.catalog.CATALOG_NAME.gcp_location=LOCATION \
--conf spark.sql.catalog.CATALOG_NAME.warehouse=WAREHOUSE_DIRECTORY

다음을 바꿉니다.


CATALOG_NAME: SQL 작업과 함께 사용하는 Spark 카탈로그의 이름
PROJECT_ID: Spark 카탈로그가 연결되는 BigLake Metastore 카탈로그의 Google Cloud 프로젝트 ID입니다.
LOCATION: BigLake Metastore의 Google Cloud 위치입니다.
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 포함된 Cloud Storage 폴더
이 값은 gs://로 시작합니다.


클러스터에 성공적으로 연결되면 Spark 터미널에 spark-sql 프롬프트가 표시됩니다.
spark-sql (default)>



BigLake Metastore 리소스 관리

이제 BigLake 메타스토어에 연결되었습니다. 기존 리소스를 보거나 BigLake Metastore에 저장된 메타데이터를 기반으로 새 리소스를 만들 수 있습니다.

예를 들어 대화형 Spark SQL 세션에서 다음 명령어를 실행하여 Iceberg 네임스페이스와 테이블을 만들어 보세요.


커스텀 Iceberg 카탈로그를 사용하세요.

USE `CATALOG_NAME`;
네임스페이스를 만듭니다.

CREATE NAMESPACE IF NOT EXISTS NAMESPACE_NAME;
생성된 네임스페이스를 사용합니다.

USE NAMESPACE_NAME;
Iceberg 테이블을 만듭니다.

CREATE TABLE TABLE_NAME (id int, data string) USING ICEBERG;
표 행을 삽입합니다.

INSERT INTO TABLE_NAME VALUES (1, "first row");
표 열을 추가합니다.

ALTER TABLE TABLE_NAME ADD COLUMNS (newDoubleCol double);
테이블 메타데이터를 보려면 다음 단계를 따르세요.

DESCRIBE EXTENDED TABLE_NAME;
네임스페이스의 테이블을 나열합니다.

SHOW TABLES;

--- 탭: Zeppelin 노트북 [https://cloud.google.com/bigquery/docs/blms-use-dataproc?hl=ko#zeppelin-%EB%85%B8%ED%8A%B8%EB%B6%81] ---
Google Cloud 콘솔에서 Dataproc 클러스터 페이지로 이동합니다.

Dataproc 클러스터로 이동 [https://console.cloud.google.com/dataproc/clusters?hl=ko] 
사용할 클러스터의 이름을 클릭합니다.

클러스터 세부정보 페이지가 열립니다.
탐색 메뉴에서 웹 인터페이스를 클릭합니다.
구성요소 게이트웨이에서 Zeppelin을 클릭합니다. Zeppelin 노트북 페이지가 열립니다.
탐색 메뉴에서 Notebook을 클릭한 다음 +Create new note를 클릭합니다.
대화상자에 노트북 이름을 입력합니다. Spark를 기본 인터프리터로 선택된 상태로 둡니다.
만들기를 클릭합니다. 새 노트북이 생성됩니다.
노트북에서 설정 메뉴를 클릭한 다음 인터프리터를 클릭합니다.
인터프리터 검색 필드에서 Spark를 검색합니다.
수정을 클릭합니다.
Spark.jars 필드에 Spark jar의 URI를 입력합니다.
https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar,gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar

저장을 클릭합니다.
확인을 클릭합니다.
다음 PySpark 코드를 Zeppelin 노트북 [https://zeppelin.apache.org/docs/0.6.0/quickstart/explorezeppelinui.html#note-layout]에 복사합니다.

%pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder \
.appName("BigLake Metastore Iceberg") \
.config("spark.sql.catalog.CATALOG_NAME", "org.apache.iceberg.spark.SparkCatalog") \
.config("spark.sql.catalog.CATALOG_NAME.catalog-impl", "org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog") \
.config("spark.sql.catalog.CATALOG_NAME.gcp_project", "PROJECT_ID") \
.config("spark.sql.catalog.CATALOG_NAME.gcp_location", "LOCATION") \
.config("spark.sql.catalog.CATALOG_NAME.warehouse", "WAREHOUSE_DIRECTORY") \
.getOrCreate()
spark.sql("select version()").show()
spark.sql("USE `CATALOG_NAME`;")
spark.sql("CREATE NAMESPACE IF NOT EXISTS NAMESPACE_NAME;")
spark.sql("USE NAMESPACE_NAME;")
spark.sql("CREATE TABLE TABLE_NAME (id int, data string) USING ICEBERG;")
spark.sql("DESCRIBE TABLE_NAME;").show()

다음을 바꿉니다.


CATALOG_NAME: SQL 작업에 사용할 Spark 카탈로그의 이름
PROJECT_ID: Dataproc 클러스터가 포함된 Google Cloud 프로젝트의 ID입니다.
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 포함된 Cloud Storage 폴더
이 값은 gs://로 시작합니다.
NAMESPACE_NAME: Spark 테이블을 참조하는 네임스페이스 이름
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 저장된 Cloud Storage 폴더의 URI
TABLE_NAME: Spark 테이블의 테이블 이름

실행 아이콘을 클릭하거나 Shift-Enter 키를 눌러 코드를 실행합니다. 작업이 완료되면 상태 메시지에 'Spark 작업 완료됨'이 표시되고 출력에는 테이블 콘텐츠가 표시됩니다.
BigLake Metastore를 Flink에 연결
다음 안내에서는 Flink SQL 클라이언트를 사용하여 Dataproc을 BigLake metastore에 연결하는 방법을 보여줍니다.
카탈로그 플러그인 설치 및 Flink 세션에 연결
BigLake Metastore를 Flink에 연결하려면 다음을 수행하세요.
선택적 Flink 구성요소를 사용 설정하여 Dataproc 클러스터를 만들고 [https://cloud.google.com/dataproc/docs/concepts/components/flink?hl=ko] Dataproc 2.2 이상을 사용하고 있는지 확인합니다.
Google Cloud 콘솔에서 VM 인스턴스 페이지로 이동합니다.
VM 인스턴스로 이동 [https://console.cloud.google.com/compute/instances?hl=ko]
가상 머신 인스턴스 목록에서 SSH를 클릭하여 Dataproc VM 인스턴스에 연결합니다.
BigLake Metastore용 Iceberg 커스텀 카탈로그 플러그인을 구성합니다.
FLINK_VERSION=1.17
ICEBERG_VERSION=1.5.2

cd /usr/lib/flink

sudo wget -c https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-flink-runtime-${FLINK_VERSION}/${ICEBERG_VERSION}/iceberg-flink-runtime-${FLINK_VERSION}-${ICEBERG_VERSION}.jar -P lib

sudo gcloud storage cp gs://spark-lib/bigquery/iceberg-bigquery-catalog-${ICEBERG_VERSION}-1.0.1-beta.jar lib/
YARN에서 Flink 세션을 시작합니다.
HADOOP_CLASSPATH=`hadoop classpath`

sudo bin/yarn-session.sh -nm flink-dataproc -d

sudo bin/sql-client.sh embedded \
  -s yarn-session
Flink에서 카탈로그를 만듭니다.
CREATE CATALOG 
CATALOG_NAME WITH (
  'type'='iceberg',
  'warehouse'='
WAREHOUSE_DIRECTORY',
  'catalog-impl'='org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog',
  'gcp_project'='
PROJECT_ID',
  'gcp_location'='
LOCATION'
);
다음을 바꿉니다.
CATALOG_NAME: BigLake Metastore 카탈로그에 연결된 Flink 카탈로그 식별자입니다.
WAREHOUSE_DIRECTORY: 웨어하우스 디렉터리의 기본 경로입니다 (Flink가 파일을 만드는 Cloud Storage 폴더). 이 값은 gs://로 시작합니다.
PROJECT_ID: Flink 카탈로그가 연결되는 BigLake Metastore 카탈로그의 프로젝트 ID입니다.
LOCATION: BigQuery 리소스의 위치 [https://cloud.google.com/bigquery/docs/locations?hl=ko]입니다.
이제 Flink 세션이 BigLake Metastore에 연결되었으며 Flink SQL 명령어를 실행할 수 있습니다.
BigLake Metastore 리소스 관리
이제 BigLake Metastore에 연결되었으므로 BigLake Metastore에 저장된 메타데이터를 기반으로 리소스를 만들고 볼 수 있습니다.
예를 들어 대화형 Flink SQL 세션에서 다음 명령어를 실행하여 Iceberg 데이터베이스와 테이블을 만듭니다.
커스텀 Iceberg 카탈로그를 사용하세요.
USE CATALOG 
CATALOG_NAME;
CATALOG_NAME을 Flink 카탈로그 식별자로 바꿉니다.
데이터베이스를 만들어 BigQuery에서 데이터 세트를 만듭니다.
CREATE DATABASE IF NOT EXISTS 
DATABASE_NAME;
DATABASE_NAME을 새 데이터베이스의 이름으로 바꿉니다.
생성한 데이터베이스를 사용합니다.
USE 
DATABASE_NAME;
Iceberg 테이블을 만듭니다. 다음은 예시 판매 테이블을 만듭니다.
CREATE TABLE IF NOT EXISTS 
ICEBERG_TABLE_NAME (
    order_number BIGINT,
    price        DECIMAL(32,2),
    buyer        ROW<first_name STRING, last_name STRING>,
    order_time   TIMESTAMP(3)
);
ICEBERG_TABLE_NAME을 새 테이블의 이름으로 바꿉니다.
테이블 메타데이터를 보려면 다음 단계를 따르세요.
DESCRIBE EXTENDED 
ICEBERG_TABLE_NAME;
데이터베이스의 테이블을 나열합니다.
SHOW TABLES;
테이블에 데이터 수집
이전 섹션에서 Iceberg 테이블을 만든 후 Flink DataGen을 데이터 소스로 사용하여 테이블에 실시간 데이터를 수집할 수 있습니다. 다음 단계는 이 워크플로의 예시입니다.
DataGen을 사용하여 임시 테이블을 만듭니다.
CREATE TEMPORARY TABLE 
DATABASE_NAME.
TEMP_TABLE_NAME
WITH (
    'connector' = 'datagen',
    'rows-per-second' = '10',
    'fields.order_number.kind' = 'sequence',
    'fields.order_number.start' = '1',
    'fields.order_number.end' = '1000000',
    'fields.price.min' = '0',
    'fields.price.max' = '10000',
    'fields.buyer.first_name.length' = '10',
    'fields.buyer.last_name.length' = '10'
)
LIKE 
DATABASE_NAME.
ICEBERG_TABLE_NAME (EXCLUDING ALL);
다음을 바꿉니다.
DATABASE_NAME: 임시 테이블을 저장할 데이터베이스의 이름입니다.
TEMP_TABLE_NAME: 임시 테이블의 이름
ICEBERG_TABLE_NAME: 이전 섹션에서 만든 Iceberg 테이블의 이름입니다.
동시 로드를 1로 설정합니다.
SET 'parallelism.default' = '1';
체크포인트 간격을 설정합니다.
SET 'execution.checkpointing.interval' = '10second';
체크포인트를 설정합니다.
SET 'state.checkpoints.dir' = 'hdfs:///flink/checkpoints';
실시간 스트리밍 작업을 시작합니다.
INSERT INTO 
ICEBERG_TABLE_NAME SELECT * FROM 
TEMP_TABLE_NAME;
출력은 다음과 비슷합니다.
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 0de23327237ad8a811d37748acd9c10b
스트리밍 작업의 상태를 확인하려면 다음 단계를 따르세요.
Google Cloud 콘솔에서 클러스터 페이지로 이동합니다.
클러스터로 이동 [https://console.cloud.google.com/dataproc/clusters?hl=ko]
클러스터를 선택합니다.
웹 인터페이스 탭을 클릭합니다.
YARN ResourceManager 링크를 클릭합니다.
YARN ResourceManager 인터페이스에서 Flink 세션을 찾아 추적 UI 아래의 ApplicationMaster 링크를 클릭합니다.
상태 열에서 작업 상태가 실행 중인지 확인합니다.
Flink SQL 클라이언트에서 스트리밍 데이터를 쿼리합니다.
SELECT * FROM 
ICEBERG_TABLE_NAME
/*+ OPTIONS('streaming'='true', 'monitor-interval'='3s')*/
ORDER BY order_time desc
LIMIT 20;
BigQuery에서 스트리밍 데이터를 쿼리합니다.
SELECT * FROM `
DATABASE_NAME.
ICEBERG_TABLE_NAME`
ORDER BY order_time desc
LIMIT 20;
Flink SQL 클라이언트에서 스트리밍 작업을 종료합니다.
STOP JOB '
JOB_ID';
JOB_ID를 스트리밍 작업을 만들 때 출력에 표시된 작업 ID로 바꿉니다.
다음 단계
선택적 BigLake Metastore 기능 [https://cloud.google.com/bigquery/docs/blms-features?hl=ko] 설정하기
BigQuery에서 Spark의 테이블을 보고 쿼리합니다 [https://cloud.google.com/bigquery/docs/blms-query-tables?hl=ko].
도움이 되었나요?
의견 보내기