Source URL: https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless

이 페이지는 Cloud Translation API [https://cloud.google.com/translate/?hl=ko]를 통해 번역되었습니다.
Switch to English
BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#before-you-begin]
필요한 역할 [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#required-roles]
일반 워크플로 [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#workflow-to-dataproc-serverless]
BigLake Metastore를 Spark와 연결 [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#connect-biglake]
다음 단계 [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#what's-next]
Dataproc Serverless에서 BigLake metastore 사용
bookmark_border
이 문서에서는 Dataproc Serverless [https://cloud.google.com/dataproc-serverless/docs/overview?hl=ko]에서 BigLake metastore를 사용하는 방법을 설명합니다.
시작하기 전에
Google Cloud 프로젝트에 결제를 사용 설정합니다. 프로젝트에 결제가 사용 설정되어 있는지 확인 [https://cloud.google.com/billing/docs/how-to/verify-billing-enabled?hl=ko]하는 방법을 알아보세요.
BigQuery 및 Dataproc API를 사용 설정합니다.
API 사용 설정 [https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com%2Cdataproc.googleapis.com&hl=ko]
선택사항: BigLake Metastore의 작동 방식 [https://cloud.google.com/bigquery/docs/about-blms?hl=ko]과 이를 사용해야 하는 이유를 알아봅니다.
필요한 역할
BigLake Metastore를 메타데이터 저장소로 사용하여 Spark 및 Dataproc Serverless를 사용하는 데 필요한 권한을 얻으려면 관리자에게 다음 IAM 역할을 부여해 달라고 요청하세요.
Spark에서 BigLake metastore 테이블을 만듭니다.
프로젝트의 Dataproc Serverless 서비스 계정에 대한 Dataproc 작업자 [https://cloud.google.com/iam/docs/roles-permissions/dataproc?hl=ko#dataproc.worker] (roles/dataproc.worker)
프로젝트의 Dataproc Serverless 서비스 계정에 대한 BigQuery 데이터 편집자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.dataEditor] (roles/bigquery.dataEditor)
프로젝트의 Dataproc Serverless 서비스 계정에 대한 스토리지 객체 관리자 [https://cloud.google.com/iam/docs/roles-permissions/storage?hl=ko#storage.objectAdmin] (roles/storage.objectAdmin)
BigQuery에서 BigLake metastore 테이블을 쿼리합니다.
프로젝트에 대한 BigQuery 데이터 뷰어 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.dataViewer] (roles/bigquery.dataViewer)
프로젝트에 대한 BigQuery 사용자 [https://cloud.google.com/iam/docs/roles-permissions/bigquery?hl=ko#bigquery.user] (roles/bigquery.user)
프로젝트의 스토리지 객체 뷰어 [https://cloud.google.com/iam/docs/roles-permissions/storage?hl=ko#storage.objectViewer] (roles/storage.objectViewer)
역할 부여에 대한 자세한 내용은 프로젝트, 폴더, 조직에 대한 액세스 관리 [https://cloud.google.com/iam/docs/granting-changing-revoking-access?hl=ko]를 참조하세요.
커스텀 역할 [https://cloud.google.com/iam/docs/creating-custom-roles?hl=ko]이나 다른 사전 정의된 역할 [https://cloud.google.com/iam/docs/roles-overview?hl=ko#predefined]을 통해 필요한 권한을 얻을 수도 있습니다.
일반 워크플로
Dataproc Serverless에서 BigQuery를 사용하려면 다음 일반 단계를 따르세요.
BigLake metastore에서 실행할 명령어로 파일을 만듭니다.
원하는 오픈소스 소프트웨어 엔진에 연결합니다.
Spark SQL 또는 PySpark와 같은 원하는 방법을 사용하여 일괄 작업을 제출합니다.
BigLake Metastore를 Spark와 연결
다음 안내에서는 Dataproc 서버리스를 BigLake Metastore에 연결하는 방법을 보여줍니다.
--- 탭: SparkSQL [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#sparksql] ---
Spark SQL 일괄 작업을 제출하려면 다음 단계를 완료하세요.


BigLake metastore에서 실행하려는 Spark SQL 명령어로 SQL 파일을 만듭니다. 예를 들어 이 명령어는 네임스페이스와 테이블을 만듭니다.

CREATE NAMESPACE `CATALOG_NAME`.NAMESPACE_NAME;
CREATE TABLE `CATALOG_NAME`.NAMESPACE_NAME.TABLE_NAME (id int, data string) USING ICEBERG LOCATION 'WAREHOUSE_DIRECTORY';

다음을 바꿉니다.


CATALOG_NAME: Spark 테이블을 참조하는 카탈로그 이름입니다.
NAMESPACE_NAME: Spark 테이블을 참조하는 네임스페이스 이름입니다.
TABLE_NAME: Spark 테이블의 테이블 이름입니다.
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 저장된 Cloud Storage 폴더의 URI입니다.

다음 gcloud dataproc batches submit spark-sql [https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/spark-sql?hl=ko] gcloud CLI 명령어를 실행하여 Spark SQL 일괄 작업을 제출합니다.
참고: BigQuery 일괄 워크로드를 실행하는 리전 VPC 서브넷에 비공개 Google 액세스 [https://cloud.google.com/vpc/docs/configure-private-google-access?hl=ko#config-pga]가 사용 설정되어 있는지 확인합니다. 선택한 리전의 '기본' 네트워크 서브넷에 아직 비공개 Google 액세스가 사용 설정되어 있지 않은 경우 --subnet 플래그를 추가하여 선택한 리전에서 PGA가 사용 설정된 다른 네트워크의 서브넷을 지정해야 합니다.
gcloud dataproc batches submit spark-sql SQL_SCRIPT_PATH \
--project=PROJECT_ID \
--region=REGION \
--subnet=projects/PROJECT_ID/regions/REGION/subnetworks/SUBNET_NAME \
--deps-bucket=BUCKET_PATH \
--properties="spark.sql.catalog.CATALOG_NAME=org.apache.iceberg.spark.SparkCatalog,spark.sql.catalog.CATALOG_NAME.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog,spark.sql.catalog.CATALOG_NAME.gcp_project=PROJECT_ID,spark.sql.catalog.CATALOG_NAME.gcp_location=LOCATION,spark.sql.catalog.CATALOG_NAME.warehouse=WAREHOUSE_DIRECTORY"

다음을 바꿉니다.


SQL_SCRIPT_PATH: 일괄 작업에서 사용하는 SQL 파일의 경로입니다.
PROJECT_ID: 일괄 작업을 실행할 Google Cloud 프로젝트의 ID입니다.
REGION: 워크로드가 실행되는 리전
SUBNET_NAME: 선택사항: REGION에 있고 비공개 Google 액세스 [https://cloud.google.com/vpc/docs/private-google-access?hl=ko]가 사용 설정되어 있으며 다른 세션 서브넷 요구사항 [https://cloud.google.com/dataproc-serverless/docs/concepts/network?hl=ko]을 충족하는 VPC 서브넷의 이름입니다.
LOCATION: 일괄 작업을 실행할 위치입니다.
BUCKET_PATH: 워크로드 종속 항목을 업로드할 Cloud Storage 버킷의 위치.
WAREHOUSE_FOLDER는 이 버킷에 있습니다.
버킷의 gs:// URI 프리픽스는 필요하지 않습니다. 버킷 경로 또는 버킷 이름을 지정할 수 있습니다(예: mybucketname1).


Spark 일괄 작업 제출에 관한 자세한 내용은 Spark 일괄 워크로드 실행 [https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch?hl=ko]을 참고하세요.

--- 탭: PySpark [https://cloud.google.com/bigquery/docs/blms-use-dataproc-serverless?hl=ko#pyspark] ---
PySpark 일괄 작업을 제출하려면 다음 단계를 완료하세요.


BigLake metastore에서 실행하려는 PySpark 명령어로 Python 파일을 만듭니다.

예를 들어 다음 명령어는 BigLake metastore에 저장된 Iceberg 테이블과 상호작용하도록 Spark 환경을 설정합니다. 그런 다음 이 명령어는 새 네임스페이스와 해당 네임스페이스 내에 Iceberg 테이블을 만듭니다.

from pyspark.sql import SparkSession

spark = SparkSession.builder \
.appName("BigLake Metastore Iceberg") \
.config("spark.sql.catalog.CATALOG_NAME", "org.apache.iceberg.spark.SparkCatalog") \
.config("spark.sql.catalog.CATALOG_NAME.catalog-impl", "org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog") \
.config("spark.sql.catalog.CATALOG_NAME.gcp_project", "PROJECT_ID") \
.config("spark.sql.catalog.CATALOG_NAME.gcp_location", "LOCATION") \
.config("spark.sql.catalog.CATALOG_NAME.warehouse", "WAREHOUSE_DIRECTORY") \
.getOrCreate()

spark.sql("USE `CATALOG_NAME`;")
spark.sql("CREATE NAMESPACE IF NOT EXISTS NAMESPACE_NAME;")
spark.sql("USE NAMESPACE_NAME;")
spark.sql("CREATE TABLE TABLE_NAME (id int, data string) USING ICEBERG LOCATION 'WAREHOUSE_DIRECTORY';")

다음을 바꿉니다.


PROJECT_ID: 일괄 작업을 실행할 Google Cloud 프로젝트의 ID입니다.
LOCATION: BigQuery 리소스가 있는 위치 [https://cloud.google.com/bigquery/docs/locations?hl=ko]입니다.
CATALOG_NAME: Spark 테이블을 참조하는 카탈로그 이름
TABLE_NAME: Spark 테이블의 테이블 이름
WAREHOUSE_DIRECTORY: 데이터 웨어하우스가 저장된 Cloud Storage 폴더의 URI입니다.
NAMESPACE_NAME: Spark 테이블을 참조하는 네임스페이스 이름입니다.

다음 gcloud dataproc batches submit pyspark [https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark?hl=ko] 명령어를 사용하여 일괄 작업을 제출합니다.

gcloud dataproc batches submit pyspark PYTHON_SCRIPT_PATH \
 --version=2.2 \
 --project=PROJECT_ID \
 --region=REGION \
 --deps-bucket=BUCKET_PATH \
 --properties="spark.sql.catalog.CATALOG_NAME=org.apache.iceberg.spark.SparkCatalog,spark.sql.catalog.CATALOG_NAME.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog,spark.sql.catalog.CATALOG_NAME.gcp_project=PROJECT_ID,spark.sql.catalog.CATALOG_NAME.gcp_location=LOCATION,spark.sql.catalog.CATALOG_NAME.warehouse=WAREHOUSE_DIRECTORY"

다음을 바꿉니다.


PYTHON_SCRIPT_PATH: 일괄 작업에서 사용하는 Python 스크립트의 경로
PROJECT_ID: 일괄 작업을 실행할 Google Cloud 프로젝트의 ID입니다.
REGION: 워크로드가 실행되는 리전
BUCKET_PATH: 워크로드 종속 항목을 업로드할 Cloud Storage 버킷의 위치.
버킷의 gs:// URI 프리픽스는 필요하지 않습니다. 버킷 경로 또는 버킷 이름을 지정할 수 있습니다(예: mybucketname1).


PySpark 일괄 작업 제출에 대한 자세한 내용은 PySpark gcloud 참조 [https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/pyspark?hl=ko]를 확인하세요.
다음 단계
선택적 BigLake Metastore 기능 [https://cloud.google.com/bigquery/docs/blms-features?hl=ko] 설정하기
BigQuery 콘솔에서 Spark의 테이블을 보고 쿼리합니다 [https://cloud.google.com/bigquery/docs/blms-query-tables?hl=ko].
도움이 되었나요?
의견 보내기