Source URL: https://cloud.google.com/bigquery/docs/migration/redshift-vpc

BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
도움이 되었나요?
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#before_you_begin]
필수 권한 설정 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#set_required_permissions]
데이터 세트 만들기 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#create_a_dataset]
Amazon Redshift 클러스터에 대한 액세스 권한 부여 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#grant_access_redshift_cluster]
Amazon S3 버킷에 대한 액세스 권한 부여 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#grant-access]
별도의 마이그레이션 큐로 워크로드 제어 구성 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#configure_workload_control_with_a_separate_migration_queue]
전송 정보 수집 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#gather_transfer_information]
데이터 평가 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#assess_your_data]
VPC 네트워크를 사용하여 Amazon Redshift 데이터 마이그레이션
bookmark_border
이 문서에서는 VPC를 사용하여 Amazon Redshift에서 BigQuery로 데이터를 마이그레이션하는 방법을 설명합니다.
AWS에 프라이빗 Amazon Redshift 인스턴스가 있는 경우 가상 프라이빗 클라우드(VPC) 네트워크 [https://cloud.google.com/vpc/docs/overview?hl=ko]를 만들고 Amazon Redshift VPC 네트워크에 연결하여 해당 데이터를 BigQuery로 마이그레이션할 수 있습니다. 데이터 마이그레이션 프로세스는 다음과 같이 작동합니다.
전송에 사용할 프로젝트에 VPC 네트워크를 만듭니다. VPC 네트워크는 공유 VPC [https://cloud.google.com/vpc/docs/shared-vpc?hl=ko] 네트워크일 수 없습니다.
가상 사설망(VPN) [https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview?hl=ko]을 설정하고 프로젝트 VPC 네트워크와 Amazon Redshift VPC 네트워크를 연결합니다.
전송을 설정할 때 프로젝트 VPC 네트워크와 예약된 IP 범위를 지정합니다.
BigQuery Data Transfer Service는 테넌트 프로젝트를 만들어 전송에 사용하는 프로젝트에 연결합니다.
BigQuery Data Transfer Service는 사용자가 지정한 예약된 IP 범위를 사용하여 테넌트 프로젝트에 하나의 서브넷이 있는 VPC 네트워크를 생성합니다.
BigQuery Data Transfer Service는 프로젝트 VPC 네트워크와 테넌트 프로젝트 VPC 네트워크 간에 VPC 피어링 [https://cloud.google.com/vpc/docs/vpc-peering?hl=ko]을 만듭니다.
BigQuery Data Transfer Service 이전은 테넌트 프로젝트에서 실행됩니다. Amazon Redshift에서 Amazon S3 버킷의 스테이징 영역으로 언로드 작업을 트리거합니다. 언로드 속도는 클러스터 구성에 따라 결정됩니다.
BigQuery Data Transfer Service 마이그레이션은 Amazon S3 버킷의 데이터를 BigQuery로 전송합니다.
주의: BigQuery와 Amazon Redshift 간의 통신은 피어링된 VPC 네트워크 간의 VPN을 통해 이루어집니다. 그러나 Amazon S3에서 BigQuery로의 데이터 이동은 공용 인터넷을 통해 이루어집니다.
퍼블릭 IP를 통해 Amazon Redshift 인스턴스에서 데이터를 전송하려면 이 지침을 사용하여 Amazon Redshift 데이터를 BigQuery로 마이그레이션 [https://cloud.google.com/bigquery/docs/migration/redshift?hl=ko]할 수 있습니다.
시작하기 전에
Sign in to your Google Cloud account. If you're new to Google Cloud, create an account [https://console.cloud.google.com/freetrial?hl=ko] to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads.
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.
Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector [https://console.cloud.google.com/projectselector2/home/dashboard?hl=ko]
Verify that billing is enabled for your Google Cloud project [https://cloud.google.com/billing/docs/how-to/verify-billing-enabled?hl=ko#confirm_billing_is_enabled_on_a_project].
Enable the BigQuery and BigQuery Data Transfer Service APIs.
Enable the APIs [https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com%2Cbigquerydatatransfer.googleapis.com&hl=ko]
필수 권한 설정
Amazon Redshift 전송을 만들기 전에 다음 단계를 수행합니다.
전송을 만드는 사람에게 다음과 같은 BigQuery 필수 Identity and Access Management(IAM) 권한이 있는지 확인합니다.
전송을 만들 bigquery.transfers.update 권한
대상 데이터 세트에 대한 bigquery.datasets.update 권한
사전 정의된 IAM 역할 role/bigquery.admin에는 bigquery.transfers.update 및 bigquery.datasets.update 권한이 있습니다. BigQuery Data Transfer Service의 IAM 역할에 대한 자세한 내용은 액세스 제어 [https://cloud.google.com/bigquery/docs/access-control?hl=ko]를 참조하세요.
Amazon S3의 문서를 참조하여 전송을 사용 설정하는 데 필요한 권한을 구성했는지 확인합니다. Amazon S3 소스 데이터에 최소한 AWS 관리 정책 AmazonS3ReadOnlyAccess [https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html#attach-managed-policy-console]가 적용되어야 합니다.
VPC 네트워크 피어링을 만들고 삭제하기 위한 적합한 IAM 권한 [https://cloud.google.com/iam/docs/understanding-roles?hl=ko]을 개별 전송 설정에 부여합니다. 서비스가 개인의 Google Cloud 사용자 인증 정보를 사용해서 VPC 피어링 연결을 만듭니다.
VPC 피어링을 생성할 수 있는 권한: compute.networks.addPeering
VPC 피어링을 삭제할 수 있는 권한: compute.networks.removePeering
roles/project.owner, roles/project.editor 및 roles/compute.networkAdmin 사전 정의된 Cloud IAM 역할에는 기본적으로 compute.networks.addPeering 및 compute.networks.removePeering 권한이 포함됩니다.
데이터 세트 만들기
데이터를 저장할 BigQuery 데이터 세트를 만듭니다 [https://cloud.google.com/bigquery/docs/datasets?hl=ko]. 테이블을 만들 필요는 없습니다.
Amazon Redshift 클러스터에 대한 액세스 권한 부여
SQL 클라이언트의 인바운드 규칙 구성 [https://docs.aws.amazon.com/redshift/latest/gsg/new-user.html#rs-gsg-authorize-cluster-access]의 안내에 따라 비공개 Amazon Redshift 클러스터의 IP 범위를 허용 목록에 추가하세요. 이후 단계에서는 전송을 설정할 때 이 VPC 네트워크에서 비공개 IP 범위를 정의합니다.
Amazon S3 버킷에 대한 액세스 권한 부여
Amazon Redshift 데이터를 BigQuery로 전송할 때 스테이징 영역으로 사용할 Amazon S3 버킷이 있어야 합니다. 자세한 내용은 Amazon 문서 [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/]를 참조하세요.
전용 Amazon IAM 사용자를 만들고 이 사용자에게만 Amazon Redshift에 대한 읽기 액세스 권한과 Amazon S3에 대한 읽기/쓰기 액세스 권한을 부여하는 것이 좋습니다. 이 단계를 수행하려면 다음 정책을 적용할 수 있습니다.
Amazon IAM 사용자 액세스 키 쌍 [https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html]을 만듭니다.
별도의 마이그레이션 큐로 워크로드 제어 구성
선택적으로 마이그레이션용 Amazon Redshift 큐를 정의 [https://docs.aws.amazon.com/redshift/latest/dg/cm-c-modifying-wlm-configuration.html]하여 마이그레이션에 사용되는 리소스를 제한하고 구분할 수 있습니다. 최대 동시 실행 쿼리 수를 사용하여 이 마이그레이션 큐를 구성할 수 있습니다. 그런 다음 특정 마이그레이션 사용자 그룹 [https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_GROUP.html]을 큐와 연결하고 BigQuery로 데이터를 전송하도록 마이그레이션을 설정할 때 이러한 사용자 인증 정보를 사용할 수 있습니다. 전송 서비스만 마이그레이션 큐에 액세스할 수 있습니다.
전송 정보 수집
BigQuery Data Transfer Service로 마이그레이션을 설정하는 데 필요한 정보를 수집합니다.
Amazon Redshift의 VPC 및 예약된 IP 범위를 가져옵니다.
이 지침을 따라 JDBC URL을 확인 [https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html#obtain-jdbc-url]하세요.
Amazon Redshift 데이터베이스에 대한 적절한 권한이 있는 사용자의 사용자 이름과 비밀번호를 가져옵니다.
Amazon S3 버킷에 대한 액세스 권한 부여 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#grant_access_to_your_amazon_s3_bucket]의 안내에 따라 AWS 액세스 키 쌍을 가져옵니다.
전송에 사용할 Amazon S3 버킷의 URI를 가져옵니다. 불필요한 비용이 발생하지 않도록 이 버킷에 대해 라이프 사이클 [https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html] 정책을 설정하는 것이 좋습니다. 모든 데이터를 BigQuery로 전송하는 데 충분한 시간을 확보하려면 만료 시간을 24시간으로 설정하는 것이 좋습니다.
데이터 평가
데이터 전송 과정에서 BigQuery Data Transfer Service는 Amazon Redshift의 데이터를 CSV 파일로 Cloud Storage에 씁니다. 이 파일에 ASCII 0 문자가 포함되어 있으면 BigQuery에 로드할 수 없습니다. 데이터를 평가하여 문제가 될 수 있는지 판단하는 것이 좋습니다. 문제가 될 경우 데이터를 Parquet 파일로 Amazon S3에 내보낸 후 BigQuery Data Transfer Service를 사용하여 이 파일을 가져오면 문제를 해결할 수 있습니다. 자세한 내용은 Amazon S3 전송 개요 [https://cloud.google.com/bigquery/docs/s3-transfer-intro?hl=ko]를 참고하세요.
VPC 네트워크 및 VPN 설정
VPC 피어링 사용 설정 권한이 있는지 확인합니다. 자세한 내용은 필요한 권한 설정 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#set_required_permissions]을 참조하세요.
이 가이드 안내 [https://cloud.google.com/network-connectivity/docs/vpn/tutorials/create-ha-vpn-connections-google-cloud-aws?hl=ko]에 따라 Google Cloud VPC 네트워크를 설정하고,Google Cloud 프로젝트의 VPC 네트워크와 Amazon Redshift VPC 네트워크 사이에 VPN을 설정하고, VPC 피어링을 사용 설정합니다.
주의: 서비스는 VPC 네트워크 이름을 VPC 피어링 연결 이름으로 사용하므로 해당 이름을 사용하는 기존 VPC 피어링 연결이 없는지 확인하세요.
VPN 연결을 허용하도록 Amazon Redshift를 구성합니다. 자세한 내용은 Amazon Redshift 클러스터 보안 그룹 [https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-security-groups.html]을 참조하세요.
Google Cloud 콘솔에서 VPC 네트워크 페이지로 이동하여Google Cloud VPC 네트워크가 Google Cloud 프로젝트에 있고 VPN을 통해 Amazon Redshift에 연결되었는지 확인합니다.
VPC 네트워크로 이동 [https://console.cloud.google.com/networking/networks/list?hl=ko]
콘솔 페이지에 모든 VPC 네트워크가 나열됩니다.
Amazon Redshift 전송 설정
다음 안내에 따라 Amazon Redshift 전송을 설정합니다.
Google Cloud 콘솔에서 BigQuery 페이지로 이동합니다.
BigQuery로 이동 [https://console.cloud.google.com/bigquery?hl=ko]
데이터 전송을 클릭합니다.
전송 만들기를 클릭합니다.
소스 유형 섹션의 소스 목록에서 마이그레이션: Amazon Redshift를 선택합니다.
전송 구성 이름 섹션에서 전송 이름(예: My migration)을 표시 이름 필드에 입력합니다. 표시 이름은 나중에 수정해야 할 경우에 대비해 전송을 식별할 수 있는 값이면 됩니다.
대상 설정 섹션의 데이터 세트 목록에서 만든 데이터 세트 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#create_a_dataset]를 선택합니다.
데이터 소스 세부정보 섹션에서 다음을 수행합니다.
Amazon Redshift의 JDBC 연결 URL에 Amazon Redshift 클러스터에 액세스하는 데 사용할 JDBC URL [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#jdbc_url]을 입력합니다.
Username of your database(데이터베이스 사용자 이름)에 마이그레이션하려는 Amazon Redshift 데이터베이스의 사용자 이름을 입력합니다.
Password of your database(데이터베이스 비밀번호)에 데이터베이스 비밀번호를 입력합니다.
참고: Amazon 사용자 인증 정보를 제공하면, BigQuery Data Transfer Service가 전송을 위해 사용자의 데이터에 액세스하는 제한된 목적으로만 에이전트 역할을 수행하는 데 동의하는 것입니다.
액세스 키 ID 및 보안 비밀 액세스 키에 S3 버킷에 대한 액세스 권한 부여 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#grant_access_to_your_S3_bucket]에서 확인한 액세스 키 쌍을 입력합니다.
Amazon S3 URI에 스테이징 영역으로 사용할 S3 버킷의 URI [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#s3_uri]를 입력합니다.
Amazon Redshift Schema에 마이그레이션하려는 Amazon Redshift Schema를 입력합니다.
Table name patterns(테이블 이름 패턴)에 스키마에서 테이블 이름 일치에 사용할 이름이나 패턴을 지정합니다. 정규식을 사용하여 패턴을 <table1Regex>;<table2Regex> 형식으로 지정할 수 있습니다. 이 패턴은 Java 정규 표현식 문법을 따라야 합니다. 예를 들면 다음과 같습니다.
lineitem;ordertb는 lineitem 및 ordertb라는 테이블과 일치합니다.
.*은 모든 테이블을 찾습니다.
지정된 스키마에서 모든 테이블을 마이그레이션하려면 이 필드를 비워 둡니다.
주의: 테이블이 매우 큰 경우 테이블을 한 번에 하나씩 전송하는 것이 좋습니다. BigQuery에는 로드 작업당 15TB의 로드 할당량 [https://cloud.google.com/bigquery/docs/migration/redshift-vpc?hl=ko#quotas_and_limits]이 있습니다.
VPC 및 예약된 IP 범위의 테넌트 프로젝트 VPC 네트워크에서 사용할 VPC 네트워크 이름과 비공개 IP 주소 범위를 지정합니다. IP 주소 범위를 CIDR 블록으로 지정합니다.
양식: VPC_network_name:CIDR (예: my_vpc:10.251.1.0/24)
10.x.x.x로 시작하는 CIDR 표기법에 표준 프라이빗 VPC 네트워크 주소 범위를 사용하세요.
IP 범위에는 10개 이상의 IP 주소가 있어야 합니다.
IP 범위는 프로젝트 VPC 네트워크 또는 Amazon Redshift VPC 네트워크의 서브넷과 겹치지 않아야 합니다.
동일한 Amazon Redshift 인스턴스에 대해 여러 전송을 구성한 경우 각 전송에서 동일한 VPC_network_name:CIDR 값을 사용해야 여러 전송이 동일한 마이그레이션 인프라를 재사용할 수 있습니다.
주의: 구성 후에는 이 CIDR 블록의 값을 변경할 수 없습니다.
선택사항: 알림 옵션 섹션에서 다음을 수행합니다.
전환을 클릭하여 이메일 알림을 사용 설정합니다. 이 옵션을 사용 설정하면 전송 실행이 실패할 때 전송 관리자에게 이메일 알림이 발송됩니다.
Pub/Sub 주제 선택에서 주제 [https://cloud.google.com/pubsub/docs/overview?hl=ko#types] 이름을 선택하거나 주제 만들기를 클릭합니다. 이 옵션은 전송에 대한 Pub/Sub 실행 알림 [https://cloud.google.com/bigquery/docs/transfer-run-notifications?hl=ko]을 구성합니다.
저장을 클릭합니다.
Google Cloud 콘솔은 이 전송에 대한 리소스 이름을 포함하여 모든 전송 설정 세부사항을 표시합니다.
할당량 및 한도
VPC 네트워크로 Amazon Redshift 비공개 인스턴스를 마이그레이션하면 단일 테넌트 인프라에서 마이그레이션 에이전트가 실행됩니다. 계산 리소스 제한으로 인해 최대 5개의 동시 전송 실행이 허용됩니다.
BigQuery에는 테이블별 로드 작업당 15TB의 로드 할당량이 있습니다. 내부적으로 Amazon Redshift는 테이블 데이터를 압축하므로 내보낸 테이블 크기는 Amazon Redshift가 보고한 테이블 크기보다 큽니다. 15TB보다 큰 테이블을 마이그레이션하려면 먼저 Cloud Customer Care [https://cloud.google.com/bigquery/docs/getting-support?hl=ko]에 문의하시기 바랍니다.
이 서비스를 사용하면 Google 외부에서 비용이 발생할 수 있습니다. 자세한 내용은 Amazon Redshift [https://aws.amazon.com/redshift/pricing/] 및 Amazon S3 [https://aws.amazon.com/s3/pricing/] 가격 책정 페이지를 참조하세요.
Amazon S3의 일관성 모델 [https://cloud.google.com/bigquery/docs/s3-transfer-intro?hl=ko#consistency_considerations]로 인해 일부 파일이 BigQuery로의 전송에 포함되지 않을 수 있습니다.
다음 단계
표준 Amazon Redshift 마이그레이션 [https://cloud.google.com/bigquery/docs/migration/redshift?hl=ko]에 대해 알아보기
BigQuery Data Transfer Service [https://cloud.google.com/bigquery/docs/dts-introduction?hl=ko] 자세히 알아보기
SQL 일괄 변환 [https://cloud.google.com/bigquery/docs/batch-sql-translator?hl=ko]으로 SQL 코드 마이그레이션
도움이 되었나요?
의견 보내기