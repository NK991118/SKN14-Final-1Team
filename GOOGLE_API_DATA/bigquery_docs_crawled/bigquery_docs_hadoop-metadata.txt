Source URL: https://cloud.google.com/bigquery/docs/hadoop-metadata

BigQuery [https://cloud.google.com/bigquery?hl=ko]
Documentation [https://cloud.google.com/bigquery/docs?hl=ko]
가이드 [https://cloud.google.com/bigquery/docs/introduction?hl=ko]
의견 보내기
이 페이지의 내용
시작하기 전에 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#before_you_begin]
자바 설치 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#install_java]
필수 권한 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#required_permissions]
dwh-migration-dumper 도구 설치 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#install-dumper]
마이그레이션을 위한 메타데이터 추출 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#extracting_metadata_for_migration]
다음 단계 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#whats_next]
마이그레이션을 위해 Apache Hive에서 메타데이터 추출
bookmark_border
프리뷰
이 기능에는 서비스별 약관 [https://cloud.google.com/terms/service-terms?hl=ko#1]의 일반 서비스 약관 섹션에 있는 'GA 이전 제공 서비스 약관'이 적용됩니다. GA 이전 기능은 '있는 그대로' 제공되며 지원이 제한될 수 있습니다. 자세한 내용은 출시 단계 설명 [https://cloud.google.com/products?hl=ko#product-launch-stages]을 참조하세요.
참고: 이 기능에 대한 지원을 받거나 의견을 제출하려면 bigquery-permission-migration-support@google.com [mailto:bigquery-permission-migration-support@google.com]으로 문의하세요.
이 문서에서는 Apache Hive 데이터 또는 권한 마이그레이션 실행 전에 dwh-migration-dumper 도구를 사용하여 필요한 메타데이터를 추출하는 방법을 보여줍니다.
이 문서에서는 다음 데이터 소스에서 메타데이터를 추출하는 방법을 다룹니다.
Apache Hive
Apache Hadoop 분산 파일 시스템(HDFS)
Apache Ranger
Cloudera Manager
Apache Hive 쿼리 로그
시작하기 전에
dwh-migration-dumper 도구를 사용하려면 다음 단계를 따르세요.
자바 설치
dwh-migration-dumper 도구를 실행하려는 서버에 Java 8 이상이 설치되어 있어야 합니다. Java가 설치되어 있지 않으면 Java 다운로드 페이지 [https://www.java.com/download/]에서 Java를 다운로드하여 설치합니다.
필수 권한
dwh-migration-dumper 도구를 소스 시스템에 연결하기 위해 지정하는 사용자 계정에는 해당 시스템에서 메타데이터를 읽을 수 있는 권한이 있어야 합니다. 이 계정에 플랫폼에 제공되는 메타데이터 리소스를 쿼리할 수 있는 적절한 역할 멤버십이 있는지 확인합니다. 예를 들어 INFORMATION_SCHEMA는 여러 플랫폼에 공통되는 메타데이터 리소스입니다.
dwh-migration-dumper 도구 설치
dwh-migration-dumper 도구를 설치하려면 다음 단계를 수행합니다.
dwh-migration-dumper 도구를 실행하려는 머신의 dwh-migration-dumper 도구 GitHub 저장소 [https://github.com/google/dwh-migration-tools/releases/latest]에서 ZIP 파일을 다운로드합니다.
dwh-migration-dumper 도구 zip 파일의 유효성을 검사하려면 SHA256SUMS.txt 파일 [https://github.com/google/dwh-migration-tools/releases/latest/download/SHA256SUMS.txt]을 다운로드하고 다음 명령어를 실행합니다.
--- 탭: Bash [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#bash] ---
sha256sum --check SHA256SUMS.txt

확인에 실패하면 문제 해결 [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#corrupted_zip_file]을 참조하세요.

--- 탭: Windows PowerShell [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#windows-powershell] ---
(Get-FileHash RELEASE_ZIP_FILENAME).Hash -eq ((Get-Content SHA256SUMS.txt) -Split " ")[0]

RELEASE_ZIP_FILENAME을 dwh-migration-dumper 명령줄 추출 도구 출시의 다운로드된 zip 파일 이름으로 바꿉니다(예: dwh-migration-tools-v1.0.52.zip).

True 결과는 체크섬 확인에 성공했음을 나타냅니다.

False 결과는 인증 오류를 나타냅니다. 체크섬 및 ZIP 파일이 동일한 출시 버전에서 다운로드되어 동일한 디렉터리에 있는지 확인합니다.
ZIP 파일의 압축을 풉니다. 추출 도구 바이너리는 ZIP 파일의 압축을 풀어서 만든 폴더의 /bin 하위 디렉터리에 있습니다.
추출 도구의 설치 경로를 포함하도록 PATH 환경 변수를 업데이트합니다.
마이그레이션을 위한 메타데이터 추출
다음 옵션 중 하나를 선택하여 데이터 소스의 메타데이터를 추출하는 방법을 알아보세요.
--- 탭: Apache Hive [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#apache-hive] ---
Apache Hive 섹션 데이터 웨어하우스에서 메타데이터 및 로그 쿼리 추출 [https://cloud.google.com/bigquery/docs/migration-assessment?hl=ko#apache-hive]의 단계를 실행하여 Apache Hive 메타데이터를 추출합니다. 그런 다음 마이그레이션 파일이 포함된 Cloud Storage 버킷에 메타데이터를 업로드할 수 있습니다.

--- 탭: HDFS [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#hdfs] ---
다음 명령어를 실행하여 dwh-migration-dumper 도구를 사용하여 HDFS에서 메타데이터를 추출합니다.
dwh-migration-dumper \
  --connector hdfs \
  --host HDFS-HOST \
  --port HDFS-PORT \
  --output gs://MIGRATION-BUCKET/hdfs-dumper-output.zip \
  --assessment \

다음을 바꿉니다.


HDFS-HOST: HDFS NameNode 호스트 이름입니다.
HDFS-PORT: HDFS NameNode 포트 번호입니다. 기본 8020 포트를 사용하는 경우 이 인수를 생략할 수 있습니다.
MIGRATION-BUCKET: 마이그레이션 파일을 저장하는 데 사용하는 Cloud Storage 버킷입니다.


이 명령어는 HDFS에서 메타데이터를 MIGRATION-BUCKET 디렉터리의 hdfs-dumper-output.zip이라는 파일로 추출합니다.

HDFS에서 메타데이터를 추출할 때는 다음과 같은 몇 가지 알려진 제한사항이 있습니다.


이 커넥터의 일부 태스크는 선택사항이고 실패할 수 있으며, 출력에 풀 스택 트레이드를 로깅합니다. 필수 태스크가 성공하고 hdfs-dumper-output.zip이 생성되면 HDFS 마이그레이션을 진행할 수 있습니다.
구성된 스레드 풀 크기가 너무 크면 추출 프로세스가 실패하거나 예상보다 느리게 실행될 수 있습니다. 이러한 문제가 발생하는 경우 명령줄 인수 --thread-pool-size를 사용하여 스레드 풀 크기를 줄이는 것이 좋습니다.

--- 탭: Apache Ranger [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#apache-ranger] ---
다음 명령어를 실행하여 dwh-migration-dumper 도구를 사용하여 Apache Ranger에서 메타데이터를 추출합니다.
dwh-migration-dumper \
  --connector ranger \
  --host RANGER-HOST \
  --port 6080 \
  --user RANGER-USER \
  --password RANGER-PASSWORD \
  --ranger-scheme RANGER-SCHEME \
  --output gs://MIGRATION-BUCKET/ranger-dumper-output.zip \
  --assessment \

다음을 바꿉니다.


RANGER-HOST: Apache Ranger 인스턴스의 호스트 이름입니다.
RANGER-USER: Apache Ranger 사용자의 사용자 이름입니다.
RANGER-PASSWORD: Apache Ranger 사용자의 비밀번호입니다.
RANGER-SCHEME: Apache Ranger가 http 또는 https를 사용하는지 지정합니다. 기본값은 http입니다.
MIGRATION-BUCKET: 마이그레이션 파일을 저장하는 데 사용하는 Cloud Storage 버킷입니다.


또한 다음과 같은 선택적인 플래그를 포함할 수 있습니다.


--kerberos-auth-for-hadoop: Apache Ranger가 기본 인증 대신 Kerberos로 보호되는 경우 --user 및 --password를 대체합니다. 이 플래그를 사용하려면 dwh-migration-dumper 도구 전에 kinit 명령어를 실행해야 합니다.
--ranger-disable-tls-validation: API에서 사용하는 https 인증서가 자체 서명된 경우 이 플래그를 포함합니다. 예를 들어 Cloudera를 사용하는 경우입니다.


이 명령어는 Apache Ranger에서 메타데이터를 MIGRATION-BUCKET 디렉터리의 ranger-dumper-output.zip이라는 파일로 추출합니다.

--- 탭: Cloudera [https://cloud.google.com/bigquery/docs/hadoop-metadata?hl=ko#cloudera] ---
다음 명령어를 실행하여 dwh-migration-dumper 도구를 사용하여 Cloudera에서 메타데이터를 추출합니다.
dwh-migration-dumper \
  --connector cloudera-manager \
  --url CLOUDERA-URL \
  --user CLOUDERA-USER \
  --password CLOUDERA-PASSWORD \
  --output gs://MIGRATION-BUCKET/cloudera-dumper-output.zip \
  --yarn-application-types APPLICATION-TYPES \
  --pagination-page-size PAGE-SIZE \
  --assessment \

다음을 바꿉니다.


CLOUDERA-URL: Cloudera Manager의 URL입니다.
CLOUDERA-USER: Cloudera 사용자의 사용자 이름입니다.
CLOUDERA-PASSWORD: Cloudera 사용자의 비밀번호입니다.
MIGRATION-BUCKET: 마이그레이션 파일을 저장하는 데 사용하는 Cloud Storage 버킷입니다.
APPLICATION-TYPES: (선택사항) Hadoop YARN의 기존 애플리케이션 유형 목록입니다. 예를 들면 SPARK, MAPREDUCE입니다.
PAGE-SIZE: Hadoop YARN API와 같은 서드 파티 서비스에서 가져오는 데이터의 양을 지정합니다(선택사항). 기본값은 1000이며, 이는 요청당 항목 1,000개를 나타냅니다.


이 명령어는 Cloudera에서 MIGRATION-BUCKET 디렉터리의 dwh-migration-cloudera.zip이라는 파일로 메타데이터를 추출합니다.

--- 탭: tabpanel-apache-hive-쿼리-로그 ---
Apache Hive 섹션의 hadoop-migration-assessment 로깅 후크를 사용하여 쿼리 로그 추출 [https://cloud.google.com/bigquery/docs/migration-assessment?hl=ko#apache-hive]에 나온 단계를 따라 Apache Hive 쿼리 로그를 추출합니다. 그런 다음 마이그레이션 파일이 포함된 Cloud Storage 버킷에 로그를 업로드할 수 있습니다.
다음 단계
Hadoop에서 추출한 메타데이터를 사용하여 다음을 수행할 수 있습니다.
Hadoop에서 권한 마이그레이션 [https://cloud.google.com/bigquery/docs/hadoop-permissions-migration?hl=ko]
Hadoop 전송 예약 [https://cloud.google.com/bigquery/docs/hadoop-transfer?hl=ko]
의견 보내기