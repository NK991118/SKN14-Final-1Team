{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 저장된 벡터 db 다운 링크 : https://drive.google.com/drive/folders/1i6RymMjAWqmAkOd3o2JhikI6sQ5JxKPj?usp=sharing",
   "id": "b53bbd7e1f41deef"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-05T02:11:54.005136Z",
     "start_time": "2025-09-05T02:10:39.990482Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# BGE-M3 모델 로드 (BAAI/bge-m3 사용)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# 지정된 폴더 경로 설정\n",
    "folder_path = './COMPANY_DATA2/data_ai'  # 처리할 폴더 경로\n",
    "\n",
    "# Chroma 벡터 DB 초기화\n",
    "db_dir = './data_ai_chroma_db'\n",
    "vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n",
    "\n",
    "# 청킹을 위한 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# 폴더 내의 텍스트 파일 처리\n",
    "def process_files_in_folder(folder_path: str, role: str):\n",
    "    # 지정된 폴더 내의 모든 텍스트 파일 찾기\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        print(f\"Processing file: {txt_file}\")\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 문서 청킹\n",
    "        chunks = text_splitter.split_text(text)\n",
    "\n",
    "        # 청크를 Document 객체로 변환\n",
    "        documents = []\n",
    "        for chunk in chunks:\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"sourcefile\": os.path.basename(txt_file),  # 파일명\n",
    "                    \"role\": role,  # 직급 권한 (폴더 이름)\n",
    "                    \"last_edit\": \"2025-08-19\"  # 마지막 수정일 고정\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "        # 배치로 문서들 추가 (올바른 메서드 사용)\n",
    "        if documents:\n",
    "            vectorstore.add_documents(documents)\n",
    "            print(f\"Added {len(documents)} chunks from {os.path.basename(txt_file)}\")\n",
    "\n",
    "# 데이터 처리 시작\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # role은 폴더 이름으로 설정\n",
    "        role = os.path.basename(folder_path)  # 폴더 이름을 role로 사용\n",
    "        process_files_in_folder(folder_path, role)\n",
    "        print(\"모든 문서가 성공적으로 벡터 데이터베이스에 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./COMPANY_DATA2/data_ai\\01_데이터_관리_&_보안__데이터_파이프라인_설계_문서.txt\n",
      "Added 4 chunks from 01_데이터_관리_&_보안__데이터_파이프라인_설계_문서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\02_데이터_관리_&_보안__데이터_접근_보안_정책.txt\n",
      "Added 4 chunks from 02_데이터_관리_&_보안__데이터_접근_보안_정책.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\03_데이터_관리_&_보안__데이터_거버넌스_정책.txt\n",
      "Added 4 chunks from 03_데이터_관리_&_보안__데이터_거버넌스_정책.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\04_데이터_관리_&_보안__데이터_보존_및_폐기_정책.txt\n",
      "Added 4 chunks from 04_데이터_관리_&_보안__데이터_보존_및_폐기_정책.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\05_데이터_관리_&_보안__수집된_데이터_및_전처리_기록서.txt\n",
      "Added 4 chunks from 05_데이터_관리_&_보안__수집된_데이터_및_전처리_기록서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\06_모델_개발_&_성능__모델_성능_평가_보고서.txt\n",
      "Added 4 chunks from 06_모델_개발_&_성능__모델_성능_평가_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\07_모델_개발_&_성능__모델_학습_결과서.txt\n",
      "Added 4 chunks from 07_모델_개발_&_성능__모델_학습_결과서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\08_모델_개발_&_성능__성능_비교표.txt\n",
      "Added 4 chunks from 08_모델_개발_&_성능__성능_비교표.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\09_모델_개발_&_성능__테스트_계획_및_결과_보고서.txt\n",
      "Added 5 chunks from 09_모델_개발_&_성능__테스트_계획_및_결과_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\10_모델_개발_&_성능__데이터_품질_점검_보고서.txt\n",
      "Added 4 chunks from 10_모델_개발_&_성능__데이터_품질_점검_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\11_팀_운영_문서__데이터팀_주간_업무_계획.txt\n",
      "Added 5 chunks from 11_팀_운영_문서__데이터팀_주간_업무_계획.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\12_팀_운영_문서__AI팀_주간_업무_계획.txt\n",
      "Added 5 chunks from 12_팀_운영_문서__AI팀_주간_업무_계획.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\13_팀_운영_문서__회의록(정기회의)__1.txt\n",
      "Added 4 chunks from 13_팀_운영_문서__회의록(정기회의)__1.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\14_팀_운영_문서__회의록(정기회의)__2.txt\n",
      "Added 4 chunks from 14_팀_운영_문서__회의록(정기회의)__2.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\15_팀_운영_문서__AI_윤리_리스크_관리_문서.txt\n",
      "Added 4 chunks from 15_팀_운영_문서__AI_윤리_리스크_관리_문서.txt\n",
      "Processing file: ./COMPANY_DATA2/data_ai\\INDEX.txt\n",
      "Added 3 chunks from INDEX.txt\n",
      "모든 문서가 성공적으로 벡터 데이터베이스에 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T02:12:43.994830Z",
     "start_time": "2025-09-05T02:12:37.463611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# BGE-M3 모델 로드 (BAAI/bge-m3 사용)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Chroma 벡터 DB 로드\n",
    "db_dir = './data_ai_chroma_db'\n",
    "vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n",
    "\n",
    "# 검색할 쿼리\n",
    "query = \"코드노바의 캐시 만료 시간은 어떤 기준으로 설정해야 하나요?\"\n",
    "\n",
    "# 벡터 DB에서 유사한 문서 검색\n",
    "results = vectorstore.similarity_search(query, k=3)  # k는 검색할 상위 결과의 수 (예: 3)\n",
    "\n",
    "# 검색된 결과 출력\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print(\"=\" * 50)\n"
   ],
   "id": "242cd543323e8026",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\playdata\\AppData\\Local\\Temp\\ipykernel_29428\\151949584.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "C:\\Users\\playdata\\AppData\\Local\\Temp\\ipykernel_29428\\151949584.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: # 데이터 관리 & 보안 | 데이터 보존 및 폐기 정책\n",
      "\n",
      "작성일: 2025-08-29\n",
      "회사: CodeNova | 대상: 데이터/AI팀\n",
      "\n",
      "---\n",
      "# 데이터 보존 및 폐기 정책\n",
      "(분류: 데이터 관리 & 보안) | 회사: CodeNova | 버전: v1.0 | 작성일: 2025-08-29\n",
      "\n",
      "---\n",
      "\n",
      "## 1. 개요 및 목적\n",
      "데이터 보존 및 폐기 정책은 CodeNova의 데이터 관리 및 보안 강화를 목표로 하며, 데이터의 수명 주기를 관리하고, 불필요한 데이터의 폐기를 통해 보안 리스크를 최소화하는 것을 목적으로 한다. 이 정책은 데이터의 보존 기간, 폐기 방법 및 책임을 명확히 하여 데이터 관리의 일관성을 확보한다.\n",
      "\n",
      "## 2. 적용 범위/대상\n",
      "이 정책은 CodeNova의 모든 데이터 및 정보 시스템에 적용되며, 데이터/AI팀을 포함한 모든 직원이 준수해야 한다. 적용되는 데이터의 유형은 다음과 같다:\n",
      "- 고객 데이터\n",
      "- 연구 및 개발 데이터\n",
      "- 운영 데이터\n",
      "- 로그 및 기록 데이터\n",
      "Metadata: {'last_edit': '2025-08-19', 'role': 'data_ai', 'sourcefile': '04_데이터_관리_&_보안__데이터_보존_및_폐기_정책.txt'}\n",
      "==================================================\n",
      "Result 2:\n",
      "Content: # 팀 운영 문서 | 회의록(정기회의) #2\n",
      "\n",
      "작성일: 2025-08-29\n",
      "회사: CodeNova | 대상: 데이터/AI팀\n",
      "\n",
      "---\n",
      "# 회의록(정기회의) #2\n",
      "(분류: 팀 운영 문서) | 회사: CodeNova | 버전: v1.0 | 작성일: 2025-08-29\n",
      "\n",
      "---\n",
      "\n",
      "## 1. 회의 개요\n",
      "- **일시**: 2025년 8월 28일, 오후 2시\n",
      "- **참석자**: 데이터/AI팀 전원\n",
      "- **의제**:\n",
      "  - 프로젝트 진행 현황 점검\n",
      "  - 데이터 품질 개선 방안 논의\n",
      "  - AI 모델 성능 평가 및 개선 계획\n",
      "\n",
      "## 2. 논의된 주요 사항\n",
      "### 2.1 프로젝트 진행 현황\n",
      "- 각 팀원이 현재 진행 중인 프로젝트에 대한 업데이트를 공유함.\n",
      "- 주요 이슈:\n",
      "  - 데이터 수집 지연\n",
      "  - 모델 학습 시간 증가\n",
      "\n",
      "### 2.2 데이터 품질 개선 방안\n",
      "- 데이터 정제 프로세스 강화 필요성 논의.\n",
      "- 중복 데이터 및 결측치 처리 방안 제안.\n",
      "Metadata: {'role': 'data_ai', 'last_edit': '2025-08-19', 'sourcefile': '14_팀_운영_문서__회의록(정기회의)__2.txt'}\n",
      "==================================================\n",
      "Result 3:\n",
      "Content: # 팀 운영 문서 | 회의록(정기회의) #1\n",
      "\n",
      "작성일: 2025-08-29\n",
      "회사: CodeNova | 대상: 데이터/AI팀\n",
      "\n",
      "---\n",
      "# 회의록(정기회의) #1\n",
      "(분류: 팀 운영 문서) | 회사: CodeNova | 버전: v1.0 | 작성일: 2025-08-29\n",
      "\n",
      "---\n",
      "\n",
      "## 1. 회의 개요\n",
      "- **일시**: 2025년 8월 29일 (금) 10:00 - 11:30\n",
      "- **참석자**: 김철수, 이영희, 박민수, 최지혜, 정하늘\n",
      "- **의제**:\n",
      "  - 데이터 분석 프로젝트 진행 상황 점검\n",
      "  - AI 모델 성능 개선 방안 논의\n",
      "  - 다음 분기 목표 설정\n",
      "\n",
      "## 2. 논의된 주요 사항\n",
      "### 2.1 데이터 분석 프로젝트 진행 상황\n",
      "- 현재 데이터 수집 및 전처리 단계 완료\n",
      "- 주요 지표: 데이터 품질 95%, 전처리 소요 시간 20% 감소\n",
      "- 문제점: 일부 데이터셋에서 결측치 발생\n",
      "Metadata: {'sourcefile': '13_팀_운영_문서__회의록(정기회의)__1.txt', 'role': 'data_ai', 'last_edit': '2025-08-19'}\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
