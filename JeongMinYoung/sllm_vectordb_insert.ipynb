{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T05:08:18.173563Z",
     "start_time": "2025-08-29T05:04:22.400378Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# BGE-M3 모델 로드 (BAAI/bge-m3 사용)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# 지정된 폴더와 내부 폴더 경로 설정\n",
    "root_dir = './COMPANY_DATA2'  # 지정된 폴더 경로 (직급별 폴더들)\n",
    "\n",
    "# Chroma 벡터 DB 초기화\n",
    "db_dir = './company_chroma_db'\n",
    "vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n",
    "\n",
    "# 청킹을 위한 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# 폴더 순회 및 파일 처리\n",
    "def process_files_in_folder(folder_path: str, role: str):\n",
    "    # 지정된 폴더 내의 모든 텍스트 파일 찾기\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        print(f\"Processing file: {txt_file}\")\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 문서 청킹\n",
    "        chunks = text_splitter.split_text(text)\n",
    "\n",
    "        # 청크를 Document 객체로 변환\n",
    "        documents = []\n",
    "        for chunk in chunks:\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"sourcefile\": os.path.basename(txt_file),  # 파일명\n",
    "                    \"role\": role,  # 직급 권한 (폴더 이름)\n",
    "                    \"last_edit\": \"2025-08-19\"  # 마지막 수정일 고정\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "        # 배치로 문서들 추가 (올바른 메서드 사용)\n",
    "        if documents:\n",
    "            vectorstore.add_documents(documents)\n",
    "            print(f\"Added {len(documents)} chunks from {os.path.basename(txt_file)}\")\n",
    "\n",
    "# 루트 폴더 내의 모든 직급 폴더 처리\n",
    "def process_root_folder(root_dir: str):\n",
    "    for role_folder in os.listdir(root_dir):\n",
    "        role_folder_path = os.path.join(root_dir, role_folder)\n",
    "        if os.path.isdir(role_folder_path):  # 내부 폴더만 처리\n",
    "            print(f\"Processing role folder: {role_folder}\")\n",
    "            process_files_in_folder(role_folder_path, role_folder)\n",
    "\n",
    "# 데이터 처리 시작\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_root_folder(root_dir)\n",
    "        print(\"모든 문서가 성공적으로 벡터 데이터베이스에 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing role folder: backend\n",
      "Processing file: ./COMPANY_DATA2\\backend\\01_backend__서비스_아키텍처_문서.txt\n",
      "Added 4 chunks from 01_backend__서비스_아키텍처_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\02_backend__보안_인증_가이드.txt\n",
      "Added 4 chunks from 02_backend__보안_인증_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\03_backend__에러_핸들링_매뉴얼.txt\n",
      "Added 4 chunks from 03_backend__에러_핸들링_매뉴얼.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\04_backend__배포_운영_가이드.txt\n",
      "Added 4 chunks from 04_backend__배포_운영_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\05_backend__데이터베이스_스키마_&_변경_이력_문서.txt\n",
      "Added 5 chunks from 05_backend__데이터베이스_스키마_&_변경_이력_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\06_backend__API_정책_문서.txt\n",
      "Added 6 chunks from 06_backend__API_정책_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\07_backend__캐시_및_세션_관리_정책_문서.txt\n",
      "Added 4 chunks from 07_backend__캐시_및_세션_관리_정책_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\08_backend__장애_대응_매뉴얼_(Runbook).txt\n",
      "Added 3 chunks from 08_backend__장애_대응_매뉴얼_(Runbook).txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\09_backend__로그_및_모니터링_표준_문서.txt\n",
      "Added 3 chunks from 09_backend__로그_및_모니터링_표준_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\10_backend__배포_롤백_정책_문서.txt\n",
      "Added 4 chunks from 10_backend__배포_롤백_정책_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\11_backend__보관_백업_정책_문서.txt\n",
      "Added 3 chunks from 11_backend__보관_백업_정책_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\12_backend__보안_사고_대응_매뉴얼.txt\n",
      "Added 4 chunks from 12_backend__보안_사고_대응_매뉴얼.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\13_meeting__백엔드팀_주간_업무_계획서.txt\n",
      "Added 3 chunks from 13_meeting__백엔드팀_주간_업무_계획서.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\14_meeting__백엔드팀_회의록(정기회의)__1.txt\n",
      "Added 3 chunks from 14_meeting__백엔드팀_회의록(정기회의)__1.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\15_meeting__백엔드팀_회의록(정기회의)__2.txt\n",
      "Added 3 chunks from 15_meeting__백엔드팀_회의록(정기회의)__2.txt\n",
      "Processing file: ./COMPANY_DATA2\\backend\\INDEX.txt\n",
      "Added 3 chunks from INDEX.txt\n",
      "Processing role folder: cto\n",
      "Processing file: ./COMPANY_DATA2\\cto\\01_인사_조직_기밀__임원_인사_보상_승계_계획_문서.txt\n",
      "Added 4 chunks from 01_인사_조직_기밀__임원_인사_보상_승계_계획_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\02_인사_조직_기밀__핵심_인재_유지_및_스카우트_전략_보고서.txt\n",
      "Added 4 chunks from 02_인사_조직_기밀__핵심_인재_유지_및_스카우트_전략_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\03_인사_조직_기밀__주요_인사(승진_이동_해고)_관리_문서.txt\n",
      "Added 4 chunks from 03_인사_조직_기밀__주요_인사(승진_이동_해고)_관리_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\04_인사_조직_기밀__조직_구조_개편안.txt\n",
      "Added 4 chunks from 04_인사_조직_기밀__조직_구조_개편안.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\05_인사_조직_기밀__핵심_인재_유출_리스크_분석_및_대응_전략.txt\n",
      "Added 4 chunks from 05_인사_조직_기밀__핵심_인재_유출_리스크_분석_및_대응_전략.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\06_팀_성과___내부_평가__내부_평가_피드백_문서_프론트엔드팀_근무_태도_및_기술_기여도_평가.txt\n",
      "Added 5 chunks from 06_팀_성과___내부_평가__내부_평가_피드백_문서_프론트엔드팀_근무_태도_및_기술_기여도_평가.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\07_팀_성과___내부_평가__내부_평가_피드백_문서_백엔드팀_근무_태도_및_리더십_평가.txt\n",
      "Added 3 chunks from 07_팀_성과___내부_평가__내부_평가_피드백_문서_백엔드팀_근무_태도_및_리더십_평가.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\08_팀_성과___내부_평가__내부_평가_피드백_문서_데이터_AI팀_근무_태도_및_연구_성과_평가.txt\n",
      "Added 5 chunks from 08_팀_성과___내부_평가__내부_평가_피드백_문서_데이터_AI팀_근무_태도_및_연구_성과_평가.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\09_팀_성과___내부_평가__팀_성과_자료_프론트엔드팀_KPI_달성률_및_성과_분석.txt\n",
      "Added 4 chunks from 09_팀_성과___내부_평가__팀_성과_자료_프론트엔드팀_KPI_달성률_및_성과_분석.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\10_팀_성과___내부_평가__팀_성과_자료_백엔드팀_프로젝트_성과_및_보상_연계_보고서.txt\n",
      "Added 3 chunks from 10_팀_성과___내부_평가__팀_성과_자료_백엔드팀_프로젝트_성과_및_보상_연계_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\11_팀_성과___내부_평가__팀_성과_자료_데이터_AI팀_R&D_성과_및_투자_대비_효과_분석.txt\n",
      "Added 4 chunks from 11_팀_성과___내부_평가__팀_성과_자료_데이터_AI팀_R&D_성과_및_투자_대비_효과_분석.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\12_보안___리스크_관리__취약점_대응_전략_&_보안_사고_대응_매뉴얼.txt\n",
      "Added 4 chunks from 12_보안___리스크_관리__취약점_대응_전략_&_보안_사고_대응_매뉴얼.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\13_보안___리스크_관리__민감_데이터_접근_키_암호화_키_관리_문서.txt\n",
      "Added 3 chunks from 13_보안___리스크_관리__민감_데이터_접근_키_암호화_키_관리_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\14_보안___리스크_관리__서비스_장애_대응_시나리오_(대규모_트래픽_데이터센터_장애).txt\n",
      "Added 5 chunks from 14_보안___리스크_관리__서비스_장애_대응_시나리오_(대규모_트래픽_데이터센터_장애).txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\15_보안___리스크_관리__내부자_위협_관리_가이드_(계정_유출_권한_남용_대응).txt\n",
      "Added 4 chunks from 15_보안___리스크_관리__내부자_위협_관리_가이드_(계정_유출_권한_남용_대응).txt\n",
      "Processing file: ./COMPANY_DATA2\\cto\\INDEX.txt\n",
      "Added 4 chunks from INDEX.txt\n",
      "Processing role folder: data_ai\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\01_데이터_관리_&_보안__데이터_파이프라인_설계_문서.txt\n",
      "Added 4 chunks from 01_데이터_관리_&_보안__데이터_파이프라인_설계_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\02_데이터_관리_&_보안__데이터_접근_보안_정책.txt\n",
      "Added 4 chunks from 02_데이터_관리_&_보안__데이터_접근_보안_정책.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\03_데이터_관리_&_보안__데이터_거버넌스_정책.txt\n",
      "Added 4 chunks from 03_데이터_관리_&_보안__데이터_거버넌스_정책.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\04_데이터_관리_&_보안__데이터_보존_및_폐기_정책.txt\n",
      "Added 4 chunks from 04_데이터_관리_&_보안__데이터_보존_및_폐기_정책.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\05_데이터_관리_&_보안__수집된_데이터_및_전처리_기록서.txt\n",
      "Added 4 chunks from 05_데이터_관리_&_보안__수집된_데이터_및_전처리_기록서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\06_모델_개발_&_성능__모델_성능_평가_보고서.txt\n",
      "Added 4 chunks from 06_모델_개발_&_성능__모델_성능_평가_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\07_모델_개발_&_성능__모델_학습_결과서.txt\n",
      "Added 4 chunks from 07_모델_개발_&_성능__모델_학습_결과서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\08_모델_개발_&_성능__성능_비교표.txt\n",
      "Added 4 chunks from 08_모델_개발_&_성능__성능_비교표.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\09_모델_개발_&_성능__테스트_계획_및_결과_보고서.txt\n",
      "Added 5 chunks from 09_모델_개발_&_성능__테스트_계획_및_결과_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\10_모델_개발_&_성능__데이터_품질_점검_보고서.txt\n",
      "Added 4 chunks from 10_모델_개발_&_성능__데이터_품질_점검_보고서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\11_팀_운영_문서__데이터팀_주간_업무_계획.txt\n",
      "Added 5 chunks from 11_팀_운영_문서__데이터팀_주간_업무_계획.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\12_팀_운영_문서__AI팀_주간_업무_계획.txt\n",
      "Added 5 chunks from 12_팀_운영_문서__AI팀_주간_업무_계획.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\13_팀_운영_문서__회의록(정기회의)__1.txt\n",
      "Added 4 chunks from 13_팀_운영_문서__회의록(정기회의)__1.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\14_팀_운영_문서__회의록(정기회의)__2.txt\n",
      "Added 4 chunks from 14_팀_운영_문서__회의록(정기회의)__2.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\15_팀_운영_문서__AI_윤리_리스크_관리_문서.txt\n",
      "Added 4 chunks from 15_팀_운영_문서__AI_윤리_리스크_관리_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\data_ai\\INDEX.txt\n",
      "Added 3 chunks from INDEX.txt\n",
      "Processing role folder: frontend\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\01_frontend__프론트엔드_아키텍처_문서.txt\n",
      "Added 4 chunks from 01_frontend__프론트엔드_아키텍처_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\02_frontend__API_연동_매뉴얼.txt\n",
      "Added 4 chunks from 02_frontend__API_연동_매뉴얼.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\03_frontend__빌드_배포_가이드.txt\n",
      "Added 3 chunks from 03_frontend__빌드_배포_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\04_frontend__코딩_컨벤션_&_스타일_가이드.txt\n",
      "Added 4 chunks from 04_frontend__코딩_컨벤션_&_스타일_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\05_frontend__UI_UX_디자인_가이드라인.txt\n",
      "Added 5 chunks from 05_frontend__UI_UX_디자인_가이드라인.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\06_frontend__테스트_전략_및_QA_가이드.txt\n",
      "Added 4 chunks from 06_frontend__테스트_전략_및_QA_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\07_frontend__보안_가이드라인.txt\n",
      "Added 4 chunks from 07_frontend__보안_가이드라인.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\08_frontend__로그_모니터링_가이드.txt\n",
      "Added 4 chunks from 08_frontend__로그_모니터링_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\09_frontend__협업_프로세스_문서.txt\n",
      "Added 3 chunks from 09_frontend__협업_프로세스_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\10_frontend__신규_입사자_온보딩_가이드.txt\n",
      "Added 4 chunks from 10_frontend__신규_입사자_온보딩_가이드.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\11_meeting__프론트엔드팀_주간_업무_계획.txt\n",
      "Added 4 chunks from 11_meeting__프론트엔드팀_주간_업무_계획.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\12_meeting__프론트엔드팀_회의록(정기회의)__1.txt\n",
      "Added 4 chunks from 12_meeting__프론트엔드팀_회의록(정기회의)__1.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\13_meeting__프론트엔드팀_회의록(정기회의)__2.txt\n",
      "Added 3 chunks from 13_meeting__프론트엔드팀_회의록(정기회의)__2.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\14_frontend__실패_사례_&_대응_기록_(Postmortem_문서).txt\n",
      "Added 3 chunks from 14_frontend__실패_사례_&_대응_기록_(Postmortem_문서).txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\15_frontend__프로젝트_제품_릴리즈_노트_관리_문서.txt\n",
      "Added 3 chunks from 15_frontend__프로젝트_제품_릴리즈_노트_관리_문서.txt\n",
      "Processing file: ./COMPANY_DATA2\\frontend\\INDEX.txt\n",
      "Added 3 chunks from INDEX.txt\n",
      "모든 문서가 성공적으로 벡터 데이터베이스에 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T05:18:56.066450Z",
     "start_time": "2025-08-29T05:18:48.611114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# BGE-M3 모델 로드 (BAAI/bge-m3 사용)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Chroma 벡터 DB 로드\n",
    "db_dir = './company_chroma_db'\n",
    "vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n",
    "\n",
    "# 검색할 쿼리\n",
    "query = \"코드노바의 캐시 만료 시간은 어떤 기준으로 설정해야 하나요?\"\n",
    "\n",
    "# 벡터 DB에서 유사한 문서 검색\n",
    "results = vectorstore.similarity_search(query, k=3)  # k는 검색할 상위 결과의 수 (예: 3)\n",
    "\n",
    "# 검색된 결과 출력\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print(\"=\" * 50)\n"
   ],
   "id": "242cd543323e8026",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\playdata\\AppData\\Local\\Temp\\ipykernel_9484\\4218577003.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "C:\\Users\\playdata\\AppData\\Local\\Temp\\ipykernel_9484\\4218577003.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=db_dir, embedding_function=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: <!-- 회사: 코드노바 | 대상: 사원(백엔드) | 작성일: 2025-08-29 -->\n",
      "# 캐시 및 세션 관리 정책 문서\n",
      "분류: backend | 회사: 코드노바 | 버전: v1.0 | 작성일: 2025-08-29\n",
      "\n",
      "## 1. 목적\n",
      "본 문서는 코드노바의 백엔드 팀에서 캐시 및 세션 관리를 효과적으로 수행하기 위한 정책을 정의합니다. 이를 통해 시스템 성능을 최적화하고 사용자 경험을 향상시키는 것을 목표로 합니다.\n",
      "\n",
      "## 2. 캐시 관리 정책\n",
      "\n",
      "### 2.1 캐시 사용 원칙\n",
      "- **데이터 일관성**: 캐시된 데이터는 항상 최신 상태를 유지해야 하며, 데이터 변경 시 적절한 캐시 무효화가 필요합니다.\n",
      "- **캐시 만료**: 각 캐시 항목은 유효 기간을 설정하여 자동으로 만료되도록 합니다. 만료 시간은 데이터의 특성에 따라 조정합니다.\n",
      "- **캐시 적중률 모니터링**: 캐시 적중률을 정기적으로 모니터링하여 성능 개선의 기회를 파악합니다.\n",
      "Metadata: {'role': 'backend', 'last_edit': '2025-08-19', 'sourcefile': '07_backend__캐시_및_세션_관리_정책_문서.txt'}\n",
      "==================================================\n",
      "Result 2:\n",
      "Content: <!-- 회사: 코드노바 | 대상: 사원(백엔드) | 작성일: 2025-08-29 -->\n",
      "# 보관·백업 정책 문서\n",
      "분류: backend | 회사: 코드노바 | 버전: v1.0 | 작성일: 2025-08-29\n",
      "\n",
      "## 1. 목적\n",
      "본 문서는 코드노바의 백엔드팀이 데이터 보관 및 백업을 효과적으로 수행하기 위한 정책을 제정하여 데이터의 안전성과 지속성을 보장하는 것을 목적으로 한다.\n",
      "\n",
      "## 2. 데이터 분류\n",
      "- **중요 데이터**: 비즈니스 운영에 필수적이며, 손실 시 심각한 영향을 미치는 데이터.\n",
      "- **일반 데이터**: 비즈니스 운영에 필요하지만, 손실 시 즉각적인 영향을 미치지 않는 데이터.\n",
      "- **임시 데이터**: 단기적으로 사용되며, 장기 보관이 필요 없는 데이터.\n",
      "\n",
      "## 3. 데이터 보관 정책\n",
      "### 3.1 데이터 보관 기간\n",
      "- **중요 데이터**: 최소 5년 이상 보관\n",
      "- **일반 데이터**: 최소 3년 이상 보관\n",
      "- **임시 데이터**: 6개월 이내 삭제\n",
      "Metadata: {'last_edit': '2025-08-19', 'role': 'backend', 'sourcefile': '11_backend__보관_백업_정책_문서.txt'}\n",
      "==================================================\n",
      "Result 3:\n",
      "Content: ## 4. 검증 포인트\n",
      "- 캐시 적중률 및 성능 지표를 정기적으로 검토합니다.\n",
      "- 세션 만료 및 보안 정책이 제대로 적용되고 있는지 점검합니다.\n",
      "- 사용자 피드백을 수집하여 캐시 및 세션 관리 정책의 효과성을 평가합니다.\n",
      "\n",
      "## 5. 결론\n",
      "캐시 및 세션 관리는 코드노바의 서비스 성능과 사용자 경험에 중요한 영향을 미칩니다. 본 정책을 바탕으로 효율적인 캐시 및 세션 관리를 통해 지속적인 개선을 이루어 나가야 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "다음 개정 제안: 캐시 및 세션 관리 정책의 구체적인 예시와 함께 사용자 피드백 반영 절차를 추가하는 것을 고려해보세요.\n",
      "Metadata: {'last_edit': '2025-08-19', 'sourcefile': '07_backend__캐시_및_세션_관리_정책_문서.txt', 'role': 'backend'}\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
